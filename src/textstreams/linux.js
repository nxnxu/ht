import { randomItemFromArray, randomLineRepeatedString } from "../utils";

const stream =
  '// SPDX-License-Identifier: GPL-2.0-only\n/*\n *  linux/kernel/signal.c\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n *\n *  1997-11-02  Modified for POSIX.1b signals by Richard Henderson\n *\n *  2003-06-02  Jim Houston - Concurrent Computer Corp.\n *    Changes to use preallocated sigqueue structures\n *    to allow signals to be sent reliably.\n */\n\n\n#include <linux/slab.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/user.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/task.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/cputime.h>\n#include <linux/file.h>\n#include <linux/fs.h>\n#include <linux/proc_fs.h>\n#include <linux/tty.h>\n#include <linux/binfmts.h>\n#include <linux/coredump.h>\n#include <linux/security.h>\n#include <linux/syscalls.h>\n#include <linux/ptrace.h>\n#include <linux/signal.h>\n#include <linux/signalfd.h>\n#include <linux/ratelimit.h>\n#include <linux/tracehook.h>\n#include <linux/capability.h>\n#include <linux/freezer.h>\n#include <linux/pid_namespace.h>\n#include <linux/nsproxy.h>\n#include <linux/user_namespace.h>\n#include <linux/uprobes.h>\n#include <linux/compat.h>\n#include <linux/cn_proc.h>\n#include <linux/compiler.h>\n#include <linux/posix-timers.h>\n#include <linux/cgroup.h>\n#include <linux/audit.h>\n\n#define CREATE_TRACE_POINTS\n\n#include <trace/events/signal.h>\n\n#include <asm/param.h>\n#include <linux/uaccess.h>\n#include <asm/unistd.h>\n#include <asm/siginfo.h>\n#include <asm/cacheflush.h>\n#include <asm/syscall.h>\n\nstatic struct kmem_cache *sigqueue_cachep;\n\nint print_fatal_signals\n__read_mostly;\n\nstatic void __user\n*\n\nsig_handler(struct task_struct *t, int sig) {\n  return t->sighand->action[sig - 1].sa.sa_handler;\n}\n\nstatic inline bool sig_handler_ignored(void __user\n\n*handler,\nint sig\n)\n{\n\nreturn handler == SIG_IGN ||\n(handler ==\nSIG_DFL &&sig_kernel_ignore(sig)\n);\n}\n\nstatic bool sig_task_ignored(struct task_struct *t, int sig, bool force) {\n  void __user\n  *handler;\n\n  handler = sig_handler(t, sig);\n\n  if (unlikely(is_global_init(t) && sig_kernel_only(sig)))\n    return true;\n\n  if (unlikely(t->signal->flags & SIGNAL_UNKILLABLE) &&\n      handler == SIG_DFL && !(force && sig_kernel_only(sig)))\n    return true;\n\n  if (unlikely((t->flags & PF_KTHREAD) &&\n               (handler == SIG_KTHREAD_KERNEL) && !force))\n    return true;\n\n  return sig_handler_ignored(handler, sig);\n}\n\nstatic bool sig_ignored(struct task_struct *t, int sig, bool force) {\n\n  if (sigismember(&t->blocked, sig) || sigismember(&t->real_blocked, sig))\n    return false;\n\n  if (t->ptrace && sig != SIGKILL)\n    return false;\n\n  return sig_task_ignored(t, sig, force);\n}\n\nstatic inline bool has_pending_signals(sigset_t *signal, sigset_t *blocked) {\n  unsigned long ready;\n  long i;\n\n  switch (_NSIG_WORDS) {\n    default:\n      for (i = _NSIG_WORDS, ready = 0; --i >= 0;)\n        ready |= signal->sig[i] & ~blocked->sig[i];\n      break;\n\n    case 4:\n      ready = signal->sig[3] & ~blocked->sig[3];\n      ready |= signal->sig[2] & ~blocked->sig[2];\n      ready |= signal->sig[1] & ~blocked->sig[1];\n      ready |= signal->sig[0] & ~blocked->sig[0];\n      break;\n\n    case 2:\n      ready = signal->sig[1] & ~blocked->sig[1];\n      ready |= signal->sig[0] & ~blocked->sig[0];\n      break;\n\n    case 1:\n      ready = signal->sig[0] & ~blocked->sig[0];\n  }\n  return ready != 0;\n}\n\n#define PENDING(p, b) has_pending_signals(&(p)->signal, (b))\n\nstatic bool recalc_sigpending_tsk(struct task_struct *t) {\n  if ((t->jobctl & (JOBCTL_PENDING_MASK | JOBCTL_TRAP_FREEZE)) ||\n      PENDING(&t->pending, &t->blocked) ||\n      PENDING(&t->signal->shared_pending, &t->blocked) ||\n      cgroup_task_frozen(t)) {\n    set_tsk_thread_flag(t, TIF_SIGPENDING);\n    return true;\n  }\n\n  return false;\n}\n\nvoid recalc_sigpending_and_wake(struct task_struct *t) {\n  if (recalc_sigpending_tsk(t))\n    signal_wake_up(t, 0);\n}\n\nvoid recalc_sigpending(void) {\n  if (!recalc_sigpending_tsk(current) && !freezing(current))\n    clear_thread_flag(TIF_SIGPENDING);\n\n}\n\nEXPORT_SYMBOL(recalc_sigpending);\n\nvoid calculate_sigpending(void) {\n  spin_lock_irq(&current->sighand->siglock);\n  set_tsk_thread_flag(current, TIF_SIGPENDING);\n  recalc_sigpending();\n  spin_unlock_irq(&current->sighand->siglock);\n}\n\n#define SYNCHRONOUS_MASK \\\n  (sigmask(SIGSEGV) | sigmask(SIGBUS) | sigmask(SIGILL) | \\\n   sigmask(SIGTRAP) | sigmask(SIGFPE) | sigmask(SIGSYS))\n\nint next_signal(struct sigpending *pending, sigset_t *mask) {\n  unsigned long i, *s, *m, x;\n  int sig = 0;\n\n  s = pending->signal.sig;\n  m = mask->sig;\n\n  x = *s & ~*m;\n  if (x) {\n    if (x & SYNCHRONOUS_MASK)\n      x &= SYNCHRONOUS_MASK;\n    sig = ffz(~x) + 1;\n    return sig;\n  }\n\n  switch (_NSIG_WORDS) {\n    default:\n      for (i = 1; i < _NSIG_WORDS; ++i) {\n        x = *++s & ~*++m;\n        if (!x)\n          continue;\n        sig = ffz(~x) + i * _NSIG_BPW + 1;\n        break;\n      }\n      break;\n\n    case 2:\n      x = s[1] & ~m[1];\n      if (!x)\n        break;\n      sig = ffz(~x) + _NSIG_BPW + 1;\n      break;\n\n    case 1:\n\n      break;\n  }\n\n  return sig;\n}\n\nstatic inline void print_dropped_signal(int sig) {\n  static DEFINE_RATELIMIT_STATE(ratelimit_state,\n  5 * HZ, 10);\n\n  if (!print_fatal_signals)\n    return;\n\n  if (!__ratelimit(&ratelimit_state))\n    return;\n\n  pr_info("%s/%d: reached RLIMIT_SIGPENDING, dropped signal %d\\n",\n          current->comm, current->pid, sig);\n}\n\nbool task_set_jobctl_pending(struct task_struct *task, unsigned long mask) {\n  BUG_ON(mask & ~(JOBCTL_PENDING_MASK | JOBCTL_STOP_CONSUME |\n                  JOBCTL_STOP_SIGMASK | JOBCTL_TRAPPING));\n  BUG_ON((mask & JOBCTL_TRAPPING) && !(mask & JOBCTL_PENDING_MASK));\n\n  if (unlikely(fatal_signal_pending(task) || (task->flags & PF_EXITING)))\n    return false;\n\n  if (mask & JOBCTL_STOP_SIGMASK)\n    task->jobctl &= ~JOBCTL_STOP_SIGMASK;\n\n  task->jobctl |= mask;\n  return true;\n}\n\nvoid task_clear_jobctl_trapping(struct task_struct *task) {\n  if (unlikely(task->jobctl & JOBCTL_TRAPPING)) {\n    task->jobctl &= ~JOBCTL_TRAPPING;\n    smp_mb();\n    wake_up_bit(&task->jobctl, JOBCTL_TRAPPING_BIT);\n  }\n}\n\nvoid task_clear_jobctl_pending(struct task_struct *task, unsigned long mask) {\n  BUG_ON(mask & ~JOBCTL_PENDING_MASK);\n\n  if (mask & JOBCTL_STOP_PENDING)\n    mask |= JOBCTL_STOP_CONSUME | JOBCTL_STOP_DEQUEUED;\n\n  task->jobctl &= ~mask;\n\n  if (!(task->jobctl & JOBCTL_PENDING_MASK))\n    task_clear_jobctl_trapping(task);\n}\n\nstatic bool task_participate_group_stop(struct task_struct *task) {\n  struct signal_struct *sig = task->signal;\n  bool consume = task->jobctl & JOBCTL_STOP_CONSUME;\n\n  WARN_ON_ONCE(!(task->jobctl & JOBCTL_STOP_PENDING));\n\n  task_clear_jobctl_pending(task, JOBCTL_STOP_PENDING);\n\n  if (!consume)\n    return false;\n\n  if (!WARN_ON_ONCE(sig->group_stop_count == 0))\n    sig->group_stop_count--;\n\n  if (!sig->group_stop_count && !(sig->flags & SIGNAL_STOP_STOPPED)) {\n    signal_set_stop_flags(sig, SIGNAL_STOP_STOPPED);\n    return true;\n  }\n  return false;\n}\n\nvoid task_join_group_stop(struct task_struct *task) {\n  unsigned long mask = current->jobctl & JOBCTL_STOP_SIGMASK;\n  struct signal_struct *sig = current->signal;\n\n  if (sig->group_stop_count) {\n    sig->group_stop_count++;\n    mask |= JOBCTL_STOP_CONSUME;\n  } else if (!(sig->flags & SIGNAL_STOP_STOPPED))\n    return;\n\n  task_set_jobctl_pending(task, mask | JOBCTL_STOP_PENDING);\n}\n\nstatic struct sigqueue *\n__sigqueue_alloc(int sig, struct task_struct *t, gfp_t gfp_flags,\n                 int override_rlimit, const unsigned int sigqueue_flags) {\n  struct sigqueue *q = NULL;\n  struct ucounts *ucounts = NULL;\n  long sigpending;\n\n  rcu_read_lock();\n  ucounts = task_ucounts(t);\n  sigpending = inc_rlimit_get_ucounts(ucounts, UCOUNT_RLIMIT_SIGPENDING);\n  rcu_read_unlock();\n  if (!sigpending)\n    return NULL;\n\n  if (override_rlimit ||\n      likely(sigpending <= task_rlimit(t, RLIMIT_SIGPENDING))) {\n    q = kmem_cache_alloc(sigqueue_cachep, gfp_flags);\n  } else {\n    print_dropped_signal(sig);\n  }\n\n  if (unlikely(q == NULL)) {\n    dec_rlimit_put_ucounts(ucounts, UCOUNT_RLIMIT_SIGPENDING);\n  } else {\n    INIT_LIST_HEAD(&q->list);\n    q->flags = sigqueue_flags;\n    q->ucounts = ucounts;\n  }\n  return q;\n}\n\nstatic void __sigqueue_free(struct sigqueue *q) {\n  if (q->flags & SIGQUEUE_PREALLOC)\n    return;\n  if (q->ucounts) {\n    dec_rlimit_put_ucounts(q->ucounts, UCOUNT_RLIMIT_SIGPENDING);\n    q->ucounts = NULL;\n  }\n  kmem_cache_free(sigqueue_cachep, q);\n}\n\nvoid flush_sigqueue(struct sigpending *queue) {\n  struct sigqueue *q;\n\n  sigemptyset(&queue->signal);\n  while (!list_empty(&queue->list)) {\n    q = list_entry(queue->list.next,\n    struct sigqueue , list);\n    list_del_init(&q->list);\n    __sigqueue_free(q);\n  }\n}\n\nvoid flush_signals(struct task_struct *t) {\n  unsigned long flags;\n\n  spin_lock_irqsave(&t->sighand->siglock, flags);\n  clear_tsk_thread_flag(t, TIF_SIGPENDING);\n  flush_sigqueue(&t->pending);\n  flush_sigqueue(&t->signal->shared_pending);\n  spin_unlock_irqrestore(&t->sighand->siglock, flags);\n}\n\nEXPORT_SYMBOL(flush_signals);\n\n#ifdef CONFIG_POSIX_TIMERS\nstatic void __flush_itimer_signals(struct sigpending *pending)\n{\n  sigset_t signal, retain;\n  struct sigqueue *q, *n;\n\n  signal = pending->signal;\n  sigemptyset(&retain);\n\n  list_for_each_entry_safe(q, n, &pending->list, list) {\n    int sig = q->info.si_signo;\n\n    if (likely(q->info.si_code != SI_TIMER)) {\n      sigaddset(&retain, sig);\n    } else {\n      sigdelset(&signal, sig);\n      list_del_init(&q->list);\n      __sigqueue_free(q);\n    }\n  }\n\n  sigorsets(&pending->signal, &signal, &retain);\n}\n\nvoid flush_itimer_signals(void)\n{\n  struct task_struct *tsk = current;\n  unsigned long flags;\n\n  spin_lock_irqsave(&tsk->sighand->siglock, flags);\n  __flush_itimer_signals(&tsk->pending);\n  __flush_itimer_signals(&tsk->signal->shared_pending);\n  spin_unlock_irqrestore(&tsk->sighand->siglock, flags);\n}\n#endif\n\nvoid ignore_signals(struct task_struct *t) {\n  int i;\n\n  for (i = 0; i < _NSIG; ++i)\n    t->sighand->action[i].sa.sa_handler = SIG_IGN;\n\n  flush_signals(t);\n}\n\nvoid\nflush_signal_handlers(struct task_struct *t, int force_default) {\n  int i;\n  struct k_sigaction *ka = &t->sighand->action[0];\n  for (i = _NSIG; i != 0; i--) {\n    if (force_default || ka->sa.sa_handler != SIG_IGN)\n      ka->sa.sa_handler = SIG_DFL;\n    ka->sa.sa_flags = 0;\n#ifdef __ARCH_HAS_SA_RESTORER\n    ka->sa.sa_restorer = NULL;\n#endif\n    sigemptyset(&ka->sa.sa_mask);\n    ka++;\n  }\n}\n\nbool unhandled_signal(struct task_struct *tsk, int sig) {\n  void __user\n  *handler = tsk->sighand->action[sig - 1].sa.sa_handler;\n  if (is_global_init(tsk))\n    return true;\n\n  if (handler != SIG_IGN && handler != SIG_DFL)\n    return false;\n\n  return !tsk->ptrace;\n}\n\nstatic void\ncollect_signal(int sig, struct sigpending *list, kernel_siginfo_t *info,\n               bool *resched_timer) {\n  struct sigqueue *q, *first = NULL;\n\n  list_for_each_entry(q, &list->list, list)\n  {\n    if (q->info.si_signo == sig) {\n      if (first)\n        goto still_pending;\n      first = q;\n    }\n  }\n\n  sigdelset(&list->signal, sig);\n\n  if (first) {\n    still_pending:\n    list_del_init(&first->list);\n    copy_siginfo(info, &first->info);\n\n    *resched_timer =\n        (first->flags & SIGQUEUE_PREALLOC) &&\n        (info->si_code == SI_TIMER) &&\n        (info->si_sys_private);\n\n    __sigqueue_free(first);\n  } else {\n    clear_siginfo(info);\n    info->si_signo = sig;\n    info->si_errno = 0;\n    info->si_code = SI_USER;\n    info->si_pid = 0;\n    info->si_uid = 0;\n  }\n}\n\nstatic int __dequeue_signal(struct sigpending *pending, sigset_t *mask,\n                            kernel_siginfo_t *info, bool *resched_timer) {\n  int sig = next_signal(pending, mask);\n\n  if (sig)\n    collect_signal(sig, pending, info, resched_timer);\n  return sig;\n}\n\nint dequeue_signal(struct task_struct *tsk, sigset_t *mask,\n                   kernel_siginfo_t *info, enum pid_type *type) {\n  bool resched_timer = false;\n  int signr;\n\n  *type = PIDTYPE_PID;\n  signr = __dequeue_signal(&tsk->pending, mask, info, &resched_timer);\n  if (!signr) {\n    *type = PIDTYPE_TGID;\n    signr = __dequeue_signal(&tsk->signal->shared_pending,\n                             mask, info, &resched_timer);\n#ifdef CONFIG_POSIX_TIMERS\n    if (unlikely(signr == SIGALRM)) {\n      struct hrtimer *tmr = &tsk->signal->real_timer;\n\n      if (!hrtimer_is_queued(tmr) &&\n          tsk->signal->it_real_incr != 0) {\n        hrtimer_forward(tmr, tmr->base->get_time(),\n            tsk->signal->it_real_incr);\n        hrtimer_restart(tmr);\n      }\n    }\n#endif\n  }\n\n  recalc_sigpending();\n  if (!signr)\n    return 0;\n\n  if (unlikely(sig_kernel_stop(signr))) {\n    current->jobctl |= JOBCTL_STOP_DEQUEUED;\n  }\n#ifdef CONFIG_POSIX_TIMERS\n  if (resched_timer) {\n    spin_unlock(&tsk->sighand->siglock);\n    posixtimer_rearm(info);\n    spin_lock(&tsk->sighand->siglock);\n\n\n    info->si_sys_private = 0;\n  }\n#endif\n  return signr;\n}\n\nEXPORT_SYMBOL_GPL(dequeue_signal);\n\nstatic int dequeue_synchronous_signal(kernel_siginfo_t *info) {\n  struct task_struct *tsk = current;\n  struct sigpending *pending = &tsk->pending;\n  struct sigqueue *q, *sync = NULL;\n\n  if (!((pending->signal.sig[0] & ~tsk->blocked.sig[0]) & SYNCHRONOUS_MASK))\n    return 0;\n\n  list_for_each_entry(q, &pending->list, list)\n  {\n\n    if ((q->info.si_code > SI_USER) &&\n        (sigmask(q->info.si_signo) & SYNCHRONOUS_MASK)) {\n      sync = q;\n      goto next;\n    }\n  }\n  return 0;\n  next:\n  list_for_each_entry_continue(q, &pending->list, list)\n  {\n    if (q->info.si_signo == sync->info.si_signo)\n      goto still_pending;\n  }\n\n  sigdelset(&pending->signal, sync->info.si_signo);\n  recalc_sigpending();\n  still_pending:\n  list_del_init(&sync->list);\n  copy_siginfo(info, &sync->info);\n  __sigqueue_free(sync);\n  return info->si_signo;\n}\n\nvoid signal_wake_up_state(struct task_struct *t, unsigned int state) {\n  set_tsk_thread_flag(t, TIF_SIGPENDING);\n  if (!wake_up_state(t, state | TASK_INTERRUPTIBLE))\n    kick_process(t);\n}\n\nstatic void flush_sigqueue_mask(sigset_t *mask, struct sigpending *s) {\n  struct sigqueue *q, *n;\n  sigset_t m;\n\n  sigandsets(&m, mask, &s->signal);\n  if (sigisemptyset(&m))\n    return;\n\n  sigandnsets(&s->signal, &s->signal, mask);\n  list_for_each_entry_safe(q, n, &s->list, list)\n  {\n    if (sigismember(mask, q->info.si_signo)) {\n      list_del_init(&q->list);\n      __sigqueue_free(q);\n    }\n  }\n}\n\nstatic inline int is_si_special(const struct kernel_siginfo *info) {\n  return info <= SEND_SIG_PRIV;\n}\n\nstatic inline bool si_fromuser(const struct kernel_siginfo *info) {\n  return info == SEND_SIG_NOINFO ||\n         (!is_si_special(info) && SI_FROMUSER(info));\n}\n\nstatic bool kill_ok_by_cred(struct task_struct *t) {\n  const struct cred *cred = current_cred();\n  const struct cred *tcred = __task_cred(t);\n\n  return uid_eq(cred->euid, tcred->suid) ||\n         uid_eq(cred->euid, tcred->uid) ||\n         uid_eq(cred->uid, tcred->suid) ||\n         uid_eq(cred->uid, tcred->uid) ||\n         ns_capable(tcred->user_ns, CAP_KILL);\n}\n\nstatic int check_kill_permission(int sig, struct kernel_siginfo *info,\n                                 struct task_struct *t) {\n  struct pid *sid;\n  int error;\n\n  if (!valid_signal(sig))\n    return -EINVAL;\n\n  if (!si_fromuser(info))\n    return 0;\n\n  error = audit_signal_info(sig, t);\n  if (error)\n    return error;\n\n  if (!same_thread_group(current, t) &&\n      !kill_ok_by_cred(t)) {\n    switch (sig) {\n      case SIGCONT:\n        sid = task_session(t);\n        if (!sid || sid == task_session(current))\n          break;\n        fallthrough;\n      default:\n        return -EPERM;\n    }\n  }\n\n  return security_task_kill(t, info, sig, NULL);\n}\n\nstatic void ptrace_trap_notify(struct task_struct *t) {\n  WARN_ON_ONCE(!(t->ptrace & PT_SEIZED));\n  assert_spin_locked(&t->sighand->siglock);\n\n  task_set_jobctl_pending(t, JOBCTL_TRAP_NOTIFY);\n  ptrace_signal_wake_up(t, t->jobctl & JOBCTL_LISTENING);\n}\n\nstatic bool prepare_signal(int sig, struct task_struct *p, bool force) {\n  struct signal_struct *signal = p->signal;\n  struct task_struct *t;\n  sigset_t flush;\n\n  if (signal->flags & SIGNAL_GROUP_EXIT) {\n    if (signal->core_state)\n      return sig == SIGKILL;\n  } else if (sig_kernel_stop(sig)) {\n    siginitset(&flush, sigmask(SIGCONT));\n    flush_sigqueue_mask(&flush, &signal->shared_pending);\n    for_each_thread(p, t)\n    flush_sigqueue_mask(&flush, &t->pending);\n  } else if (sig == SIGCONT) {\n    unsigned int why;\n    siginitset(&flush, SIG_KERNEL_STOP_MASK);\n    flush_sigqueue_mask(&flush, &signal->shared_pending);\n    for_each_thread(p, t)\n    {\n      flush_sigqueue_mask(&flush, &t->pending);\n      task_clear_jobctl_pending(t, JOBCTL_STOP_PENDING);\n      if (likely(!(t->ptrace & PT_SEIZED)))\n        wake_up_state(t, __TASK_STOPPED);\n      else\n        ptrace_trap_notify(t);\n    }\n\n    why = 0;\n    if (signal->flags & SIGNAL_STOP_STOPPED)\n      why |= SIGNAL_CLD_CONTINUED;\n    else if (signal->group_stop_count)\n      why |= SIGNAL_CLD_STOPPED;\n\n    if (why) {\n      signal_set_stop_flags(signal, why | SIGNAL_STOP_CONTINUED);\n      signal->group_stop_count = 0;\n      signal->group_exit_code = 0;\n    }\n  }\n\n  return !sig_ignored(p, sig, force);\n}\n\nstatic inline bool wants_signal(int sig, struct task_struct *p) {\n  if (sigismember(&p->blocked, sig))\n    return false;\n\n  if (p->flags & PF_EXITING)\n    return false;\n\n  if (sig == SIGKILL)\n    return true;\n\n  if (task_is_stopped_or_traced(p))\n    return false;\n\n  return task_curr(p) || !task_sigpending(p);\n}\n\nstatic void\ncomplete_signal(int sig, struct task_struct *p, enum pid_type type) {\n  struct signal_struct *signal = p->signal;\n  struct task_struct *t;\n\n  if (wants_signal(sig, p))\n    t = p;\n  else if ((type == PIDTYPE_PID) || thread_group_empty(p))\n    return;\n  else {\n    t = signal->curr_target;\n    while (!wants_signal(sig, t)) {\n      t = next_thread(t);\n      if (t == signal->curr_target)\n        return;\n    }\n    signal->curr_target = t;\n  }\n\n  if (sig_fatal(p, sig) &&\n      (signal->core_state || !(signal->flags & SIGNAL_GROUP_EXIT)) &&\n      !sigismember(&t->real_blocked, sig) &&\n      (sig == SIGKILL || !p->ptrace)) {\n    if (!sig_kernel_coredump(sig)) {\n      signal->flags = SIGNAL_GROUP_EXIT;\n      signal->group_exit_code = sig;\n      signal->group_stop_count = 0;\n      t = p;\n      do {\n        task_clear_jobctl_pending(t, JOBCTL_PENDING_MASK);\n        sigaddset(&t->pending.signal, SIGKILL);\n        signal_wake_up(t, 1);\n      }\n      while_each_thread(p, t);\n      return;\n    }\n  }\n\n  signal_wake_up(t, sig == SIGKILL);\n  return;\n}\n\nstatic inline bool legacy_queue(struct sigpending *signals, int sig) {\n  return (sig < SIGRTMIN) && sigismember(&signals->signal, sig);\n}\n\nstatic int\n__send_signal(int sig, struct kernel_siginfo *info, struct task_struct *t,\n              enum pid_type type, bool force) {\n  struct sigpending *pending;\n  struct sigqueue *q;\n  int override_rlimit;\n  int ret = 0, result;\n\n  assert_spin_locked(&t->sighand->siglock);\n\n  result = TRACE_SIGNAL_IGNORED;\n  if (!prepare_signal(sig, t, force))\n    goto ret;\n\n  pending = (type != PIDTYPE_PID) ? &t->signal->shared_pending : &t->pending;\n  result = TRACE_SIGNAL_ALREADY_PENDING;\n  if (legacy_queue(pending, sig))\n    goto ret;\n\n  result = TRACE_SIGNAL_DELIVERED;\n\n  if ((sig == SIGKILL) || (t->flags & PF_KTHREAD))\n    goto out_set;\n\n  if (sig < SIGRTMIN)\n    override_rlimit = (is_si_special(info) || info->si_code >= 0);\n  else\n    override_rlimit = 0;\n\n  q = __sigqueue_alloc(sig, t, GFP_ATOMIC, override_rlimit, 0);\n\n  if (q) {\n    list_add_tail(&q->list, &pending->list);\n    switch ((unsigned long) info) {\n      case (unsigned long) SEND_SIG_NOINFO:\n        clear_siginfo(&q->info);\n        q->info.si_signo = sig;\n        q->info.si_errno = 0;\n        q->info.si_code = SI_USER;\n        q->info.si_pid = task_tgid_nr_ns(current,\n                                         task_active_pid_ns(t));\n        rcu_read_lock();\n        q->info.si_uid =\n            from_kuid_munged(task_cred_xxx(t, user_ns),\n                             current_uid());\n        rcu_read_unlock();\n        break;\n      case (unsigned long) SEND_SIG_PRIV:\n        clear_siginfo(&q->info);\n        q->info.si_signo = sig;\n        q->info.si_errno = 0;\n        q->info.si_code = SI_KERNEL;\n        q->info.si_pid = 0;\n        q->info.si_uid = 0;\n        break;\n      default:\n        copy_siginfo(&q->info, info);\n        break;\n    }\n  } else if (!is_si_special(info) &&\n             sig >= SIGRTMIN && info->si_code != SI_USER) {\n    result = TRACE_SIGNAL_OVERFLOW_FAIL;\n    ret = -EAGAIN;\n    goto ret;\n  } else {\n    result = TRACE_SIGNAL_LOSE_INFO;\n  }\n\n  out_set:\n  signalfd_notify(t, sig);\n  sigaddset(&pending->signal, sig);\n\n  if (type > PIDTYPE_TGID) {\n    struct multiprocess_signals *delayed;\n    hlist_for_each_entry(delayed, &t->signal->multiprocess, node)\n    {\n      sigset_t *signal = &delayed->signal;\n\n      if (sig == SIGCONT)\n        sigdelsetmask(signal, SIG_KERNEL_STOP_MASK);\n      else if (sig_kernel_stop(sig))\n        sigdelset(signal, SIGCONT);\n      sigaddset(signal, sig);\n    }\n  }\n\n  complete_signal(sig, t, type);\n  ret:\n  trace_signal_generate(sig, info, t, type != PIDTYPE_PID, result);\n  return ret;\n}\n\nstatic inline bool has_si_pid_and_uid(struct kernel_siginfo *info) {\n  bool ret = false;\n  switch (siginfo_layout(info->si_signo, info->si_code)) {\n    case SIL_KILL:\n    case SIL_CHLD:\n    case SIL_RT:\n      ret = true;\n      break;\n    case SIL_TIMER:\n    case SIL_POLL:\n    case SIL_FAULT:\n    case SIL_FAULT_TRAPNO:\n    case SIL_FAULT_MCEERR:\n    case SIL_FAULT_BNDERR:\n    case SIL_FAULT_PKUERR:\n    case SIL_FAULT_PERF_EVENT:\n    case SIL_SYS:\n      ret = false;\n      break;\n  }\n  return ret;\n}\n\nstatic int\nsend_signal(int sig, struct kernel_siginfo *info, struct task_struct *t,\n            enum pid_type type) {\n\n  bool force = false;\n\n  if (info == SEND_SIG_NOINFO) {\n\n    force = !task_pid_nr_ns(current, task_active_pid_ns(t));\n  } else if (info == SEND_SIG_PRIV) {\n\n    force = true;\n  } else if (has_si_pid_and_uid(info)) {\n\n    struct user_namespace *t_user_ns;\n\n    rcu_read_lock();\n    t_user_ns = task_cred_xxx(t, user_ns);\n    if (current_user_ns() != t_user_ns) {\n      kuid_t uid = make_kuid(current_user_ns(), info->si_uid);\n      info->si_uid = from_kuid_munged(t_user_ns, uid);\n    }\n    rcu_read_unlock();\n\n    force = (info->si_code == SI_KERNEL);\n\n    if (!task_pid_nr_ns(current, task_active_pid_ns(t))) {\n      info->si_pid = 0;\n      force = true;\n    }\n  }\n  return __send_signal(sig, info, t, type, force);\n}\n\nstatic void print_fatal_signal(int signr) {\n  struct pt_regs *regs = signal_pt_regs();\n  pr_info("potentially unexpected fatal signal %d.\\n", signr);\n\n#if defined(__i386__) && !defined(__arch_um__)\n  pr_info("code at %08lx: ", regs->ip);\n  {\n    int i;\n    for (i = 0; i < 16; i++) {\n      unsigned char insn;\n\n      if (get_user(insn, (unsigned char *)(regs->ip + i)))\n        break;\n      pr_cont("%02x ", insn);\n    }\n  }\n  pr_cont("\\n");\n#endif\n  preempt_disable();\n  show_regs(regs);\n  preempt_enable();\n}\n\nstatic int __init\n\nsetup_print_fatal_signals(char *str) {\n  get_option(&str, &print_fatal_signals);\n\n  return 1;\n}\n\n__setup("print-fatal-signals=", setup_print_fatal_signals);\n\nint\n__group_send_sig_info(int sig, struct kernel_siginfo *info,\n                      struct task_struct *p) {\n  return send_signal(sig, info, p, PIDTYPE_TGID);\n}\n\nint\ndo_send_sig_info(int sig, struct kernel_siginfo *info, struct task_struct *p,\n                 enum pid_type type) {\n  unsigned long flags;\n  int ret = -ESRCH;\n\n  if (lock_task_sighand(p, &flags)) {\n    ret = send_signal(sig, info, p, type);\n    unlock_task_sighand(p, &flags);\n  }\n\n  return ret;\n}\n\nenum sig_handler {\n  HANDLER_CURRENT,\n  HANDLER_SIG_DFL,\n  HANDLER_EXIT,\n};\n\n#ifdef CONFIG_RT_DELAYED_SIGNALS\nstatic inline bool force_sig_delayed(struct kernel_siginfo *info,\n             struct task_struct *t)\n{\n  if (!in_atomic())\n    return false;\n\n  if (WARN_ON_ONCE(t->forced_info.si_signo))\n    return true;\n\n  if (is_si_special(info)) {\n    WARN_ON_ONCE(info != SEND_SIG_PRIV);\n    t->forced_info.si_signo = info->si_signo;\n    t->forced_info.si_errno = 0;\n    t->forced_info.si_code = SI_KERNEL;\n    t->forced_info.si_pid = 0;\n    t->forced_info.si_uid = 0;\n  } else {\n    t->forced_info = *info;\n  }\n  set_tsk_thread_flag(t, TIF_NOTIFY_RESUME);\n  return true;\n}\n#else\n\nstatic inline bool force_sig_delayed(struct kernel_siginfo *info,\n                                     struct task_struct *t) {\n  return false;\n}\n\n#endif\n\nstatic int\nforce_sig_info_to_task(struct kernel_siginfo *info, struct task_struct *t,\n                       enum sig_handler handler) {\n  unsigned long int flags;\n  int ret, blocked, ignored;\n  struct k_sigaction *action;\n  int sig = info->si_signo;\n\n  if (force_sig_delayed(info, t))\n    return 0;\n\n  spin_lock_irqsave(&t->sighand->siglock, flags);\n  action = &t->sighand->action[sig - 1];\n  ignored = action->sa.sa_handler == SIG_IGN;\n  blocked = sigismember(&t->blocked, sig);\n  if (blocked || ignored || (handler != HANDLER_CURRENT)) {\n    action->sa.sa_handler = SIG_DFL;\n    if (handler == HANDLER_EXIT)\n      action->sa.sa_flags |= SA_IMMUTABLE;\n    if (blocked) {\n      sigdelset(&t->blocked, sig);\n      recalc_sigpending_and_wake(t);\n    }\n  }\n\n  if (action->sa.sa_handler == SIG_DFL &&\n      (!t->ptrace || (handler == HANDLER_EXIT)))\n    t->signal->flags &= ~SIGNAL_UNKILLABLE;\n  ret = send_signal(sig, info, t, PIDTYPE_PID);\n  spin_unlock_irqrestore(&t->sighand->siglock, flags);\n\n  return ret;\n}\n\nint force_sig_info(struct kernel_siginfo *info) {\n  return force_sig_info_to_task(info, current, HANDLER_CURRENT);\n}\n\nint zap_other_threads(struct task_struct *p) {\n  struct task_struct *t = p;\n  int count = 0;\n\n  p->signal->group_stop_count = 0;\n\n  while_each_thread(p, t)\n  {\n    task_clear_jobctl_pending(t, JOBCTL_PENDING_MASK);\n    count++;\n\n    if (t->exit_state)\n      continue;\n    sigaddset(&t->pending.signal, SIGKILL);\n    signal_wake_up(t, 1);\n  }\n\n  return count;\n}\n\nstruct sighand_struct *__lock_task_sighand(struct task_struct *tsk,\n                                           unsigned long *flags) {\n  struct sighand_struct *sighand;\n\n  rcu_read_lock();\n  for (;;) {\n    sighand = rcu_dereference(tsk->sighand);\n    if (unlikely(sighand == NULL))\n      break;\n\n    spin_lock_irqsave(&sighand->siglock, *flags);\n    if (likely(sighand == rcu_access_pointer(tsk->sighand)))\n      break;\n    spin_unlock_irqrestore(&sighand->siglock, *flags);\n  }\n  rcu_read_unlock();\n\n  return sighand;\n}\n\n#ifdef CONFIG_LOCKDEP\nvoid lockdep_assert_task_sighand_held(struct task_struct *task)\n{\n  struct sighand_struct *sighand;\n\n  rcu_read_lock();\n  sighand = rcu_dereference(task->sighand);\n  if (sighand)\n    lockdep_assert_held(&sighand->siglock);\n  else\n    WARN_ON_ONCE(1);\n  rcu_read_unlock();\n}\n#endif\n\nint group_send_sig_info(int sig, struct kernel_siginfo *info,\n                        struct task_struct *p, enum pid_type type) {\n  int ret;\n\n  rcu_read_lock();\n  ret = check_kill_permission(sig, info, p);\n  rcu_read_unlock();\n\n  if (!ret && sig)\n    ret = do_send_sig_info(sig, info, p, type);\n\n  return ret;\n}\n\nint __kill_pgrp_info(int sig, struct kernel_siginfo *info, struct pid *pgrp) {\n  struct task_struct *p = NULL;\n  int retval, success;\n\n  success = 0;\n  retval = -ESRCH;\n  do_each_pid_task(pgrp, PIDTYPE_PGID, p)\n  {\n    int err = group_send_sig_info(sig, info, p, PIDTYPE_PGID);\n    success |= !err;\n    retval = err;\n  }\n  while_each_pid_task(pgrp, PIDTYPE_PGID, p);\n  return success ? 0 : retval;\n}\n\nint kill_pid_info(int sig, struct kernel_siginfo *info, struct pid *pid) {\n  int error = -ESRCH;\n  struct task_struct *p;\n\n  for (;;) {\n    rcu_read_lock();\n    p = pid_task(pid, PIDTYPE_PID);\n    if (p)\n      error = group_send_sig_info(sig, info, p, PIDTYPE_TGID);\n    rcu_read_unlock();\n    if (likely(!p || error != -ESRCH))\n      return error;\n\n  }\n}\n\nstatic int kill_proc_info(int sig, struct kernel_siginfo *info, pid_t pid) {\n  int error;\n  rcu_read_lock();\n  error = kill_pid_info(sig, info, find_vpid(pid));\n  rcu_read_unlock();\n  return error;\n}\n\nstatic inline bool kill_as_cred_perm(const struct cred *cred,\n                                     struct task_struct *target) {\n  const struct cred *pcred = __task_cred(target);\n\n  return uid_eq(cred->euid, pcred->suid) ||\n         uid_eq(cred->euid, pcred->uid) ||\n         uid_eq(cred->uid, pcred->suid) ||\n         uid_eq(cred->uid, pcred->uid);\n}\n\nint kill_pid_usb_asyncio(int sig, int errno, sigval_t addr,\n                         struct pid *pid, const struct cred *cred) {\n  struct kernel_siginfo info;\n  struct task_struct *p;\n  unsigned long flags;\n  int ret = -EINVAL;\n\n  if (!valid_signal(sig))\n    return ret;\n\n  clear_siginfo(&info);\n  info.si_signo = sig;\n  info.si_errno = errno;\n  info.si_code = SI_ASYNCIO;\n  *((sigval_t * ) & info.si_pid) = addr;\n\n  rcu_read_lock();\n  p = pid_task(pid, PIDTYPE_PID);\n  if (!p) {\n    ret = -ESRCH;\n    goto out_unlock;\n  }\n  if (!kill_as_cred_perm(cred, p)) {\n    ret = -EPERM;\n    goto out_unlock;\n  }\n  ret = security_task_kill(p, &info, sig, cred);\n  if (ret)\n    goto out_unlock;\n\n  if (sig) {\n    if (lock_task_sighand(p, &flags)) {\n      ret = __send_signal(sig, &info, p, PIDTYPE_TGID, false);\n      unlock_task_sighand(p, &flags);\n    } else\n      ret = -ESRCH;\n  }\n  out_unlock:\n  rcu_read_unlock();\n  return ret;\n}\n\nEXPORT_SYMBOL_GPL(kill_pid_usb_asyncio);\n\nstatic int\nkill_something_info(int sig, struct kernel_siginfo *info, pid_t pid) {\n  int ret;\n\n  if (pid > 0)\n    return kill_proc_info(sig, info, pid);\n\n  if (pid == INT_MIN)\n    return -ESRCH;\n\n  read_lock(&tasklist_lock);\n  if (pid != -1) {\n    ret = __kill_pgrp_info(sig, info,\n                           pid ? find_vpid(-pid) : task_pgrp(current));\n  } else {\n    int retval = 0, count = 0;\n    struct task_struct *p;\n\n    for_each_process(p)\n    {\n      if (task_pid_vnr(p) > 1 &&\n          !same_thread_group(p, current)) {\n        int err = group_send_sig_info(sig, info, p,\n                                      PIDTYPE_MAX);\n        ++count;\n        if (err != -EPERM)\n          retval = err;\n      }\n    }\n    ret = count ? retval : -ESRCH;\n  }\n  read_unlock(&tasklist_lock);\n\n  return ret;\n}\n\nint send_sig_info(int sig, struct kernel_siginfo *info, struct task_struct *p) {\n\n  if (!valid_signal(sig))\n    return -EINVAL;\n\n  return do_send_sig_info(sig, info, p, PIDTYPE_PID);\n}\n\nEXPORT_SYMBOL(send_sig_info);\n\n#define __si_special(priv) \\\n  ((priv) ? SEND_SIG_PRIV : SEND_SIG_NOINFO)\n\nint\nsend_sig(int sig, struct task_struct *p, int priv) {\n  return send_sig_info(sig, __si_special(priv), p);\n}\n\nEXPORT_SYMBOL(send_sig);\n\nvoid force_sig(int sig) {\n  struct kernel_siginfo info;\n\n  clear_siginfo(&info);\n  info.si_signo = sig;\n  info.si_errno = 0;\n  info.si_code = SI_KERNEL;\n  info.si_pid = 0;\n  info.si_uid = 0;\n  force_sig_info(&info);\n}\n\nEXPORT_SYMBOL(force_sig);\n\nvoid force_fatal_sig(int sig) {\n  struct kernel_siginfo info;\n\n  clear_siginfo(&info);\n  info.si_signo = sig;\n  info.si_errno = 0;\n  info.si_code = SI_KERNEL;\n  info.si_pid = 0;\n  info.si_uid = 0;\n  force_sig_info_to_task(&info, current, HANDLER_SIG_DFL);\n}\n\nvoid force_exit_sig(int sig) {\n  struct kernel_siginfo info;\n\n  clear_siginfo(&info);\n  info.si_signo = sig;\n  info.si_errno = 0;\n  info.si_code = SI_KERNEL;\n  info.si_pid = 0;\n  info.si_uid = 0;\n  force_sig_info_to_task(&info, current, HANDLER_EXIT);\n}\n\nvoid force_sigsegv(int sig) {\n  if (sig == SIGSEGV)\n    force_fatal_sig(SIGSEGV);\n  else\n    force_sig(SIGSEGV);\n}\n\nint force_sig_fault_to_task(int sig, int code, void __user\n\n*\n\naddr\n    ___ARCH_SI_IA64(int imm, unsigned int flags, unsigned long isr)\n,\n\nstruct task_struct *t\n)\n{\nstruct kernel_siginfo info;\n\nclear_siginfo(&info);\ninfo.\nsi_signo = sig;\ninfo.\nsi_errno = 0;\ninfo.\nsi_code = code;\ninfo.\nsi_addr = addr;\n#ifdef __ia64__\ninfo.si_imm = imm;\ninfo.si_flags = flags;\ninfo.si_isr = isr;\n#endif\nreturn\nforce_sig_info_to_task(&info, t, HANDLER_CURRENT\n);\n}\n\nint force_sig_fault(int sig, int code, void __user\n\n*\n\naddr\n___ARCH_SI_IA64(int imm, unsigned int flags, unsigned long isr)\n\n)\n{\nreturn\nforce_sig_fault_to_task(sig, code, addr\n___ARCH_SI_IA64(imm, flags, isr\n), current);\n}\n\nint send_sig_fault(int sig, int code, void __user\n\n*\n\naddr\n    ___ARCH_SI_IA64(int imm, unsigned int flags, unsigned long isr)\n,\n\nstruct task_struct *t\n)\n{\nstruct kernel_siginfo info;\n\nclear_siginfo(&info);\ninfo.\nsi_signo = sig;\ninfo.\nsi_errno = 0;\ninfo.\nsi_code = code;\ninfo.\nsi_addr = addr;\n#ifdef __ia64__\ninfo.si_imm = imm;\ninfo.si_flags = flags;\ninfo.si_isr = isr;\n#endif\nreturn\nsend_sig_info(info\n.si_signo, &info, t);\n}\n\nint force_sig_mceerr(int code, void __user\n\n*addr,\nshort lsb\n)\n{\nstruct kernel_siginfo info;\n\nWARN_ON((code\n!= BUS_MCEERR_AO) && (code != BUS_MCEERR_AR));\nclear_siginfo(&info);\ninfo.\nsi_signo = SIGBUS;\ninfo.\nsi_errno = 0;\ninfo.\nsi_code = code;\ninfo.\nsi_addr = addr;\ninfo.\nsi_addr_lsb = lsb;\nreturn\nforce_sig_info(&info);\n}\n\nint send_sig_mceerr(int code, void __user\n\n*addr,\nshort lsb,\nstruct task_struct *t\n)\n{\nstruct kernel_siginfo info;\n\nWARN_ON((code\n!= BUS_MCEERR_AO) && (code != BUS_MCEERR_AR));\nclear_siginfo(&info);\ninfo.\nsi_signo = SIGBUS;\ninfo.\nsi_errno = 0;\ninfo.\nsi_code = code;\ninfo.\nsi_addr = addr;\ninfo.\nsi_addr_lsb = lsb;\nreturn\nsend_sig_info(info\n.si_signo, &info, t);\n}\nEXPORT_SYMBOL(send_sig_mceerr);\n\nint force_sig_bnderr(void __user\n\n*addr,\nvoid __user\n*lower,\nvoid __user\n*upper)\n{\nstruct kernel_siginfo info;\n\nclear_siginfo(&info);\ninfo.\nsi_signo = SIGSEGV;\ninfo.\nsi_errno = 0;\ninfo.\nsi_code = SEGV_BNDERR;\ninfo.\nsi_addr = addr;\ninfo.\nsi_lower = lower;\ninfo.\nsi_upper = upper;\nreturn\nforce_sig_info(&info);\n}\n\n#ifdef SEGV_PKUERR\nint force_sig_pkuerr(void __user *addr, u32 pkey)\n{\n  struct kernel_siginfo info;\n\n  clear_siginfo(&info);\n  info.si_signo = SIGSEGV;\n  info.si_errno = 0;\n  info.si_code = SEGV_PKUERR;\n  info.si_addr = addr;\n  info.si_pkey = pkey;\n  return force_sig_info(&info);\n}\n#endif\n\nint force_sig_perf(void __user\n\n*addr,\nu32 type, u64\nsig_data)\n{\nstruct kernel_siginfo info;\n\nclear_siginfo(&info);\ninfo.\nsi_signo = SIGTRAP;\ninfo.\nsi_errno = 0;\ninfo.\nsi_code = TRAP_PERF;\ninfo.\nsi_addr = addr;\ninfo.\nsi_perf_data = sig_data;\ninfo.\nsi_perf_type = type;\n\nreturn\nforce_sig_info(&info);\n}\n\nint force_sig_seccomp(int syscall, int reason, bool force_coredump) {\n  struct kernel_siginfo info;\n\n  clear_siginfo(&info);\n  info.si_signo = SIGSYS;\n  info.si_code = SYS_SECCOMP;\n  info.si_call_addr = (void\n  __user *)KSTK_EIP(current);\n  info.si_errno = reason;\n  info.si_arch = syscall_get_arch(current);\n  info.si_syscall = syscall;\n  return force_sig_info_to_task(&info, current,\n                                force_coredump ? HANDLER_EXIT\n                                               : HANDLER_CURRENT);\n}\n\nint force_sig_ptrace_errno_trap(int errno, void __user\n\n*addr)\n{\nstruct kernel_siginfo info;\n\nclear_siginfo(&info);\ninfo.\nsi_signo = SIGTRAP;\ninfo.\nsi_errno = errno;\ninfo.\nsi_code = TRAP_HWBKPT;\ninfo.\nsi_addr = addr;\nreturn\nforce_sig_info(&info);\n}\n\nint force_sig_fault_trapno(int sig, int code, void __user\n\n*addr,\nint trapno\n)\n{\nstruct kernel_siginfo info;\n\nclear_siginfo(&info);\ninfo.\nsi_signo = sig;\ninfo.\nsi_errno = 0;\ninfo.\nsi_code = code;\ninfo.\nsi_addr = addr;\ninfo.\nsi_trapno = trapno;\nreturn\nforce_sig_info(&info);\n}\n\nint send_sig_fault_trapno(int sig, int code, void __user\n\n*addr,\nint trapno,\nstruct task_struct *t\n)\n{\nstruct kernel_siginfo info;\n\nclear_siginfo(&info);\ninfo.\nsi_signo = sig;\ninfo.\nsi_errno = 0;\ninfo.\nsi_code = code;\ninfo.\nsi_addr = addr;\ninfo.\nsi_trapno = trapno;\nreturn\nsend_sig_info(info\n.si_signo, &info, t);\n}\n\nint kill_pgrp(struct pid *pid, int sig, int priv) {\n  int ret;\n\n  read_lock(&tasklist_lock);\n  ret = __kill_pgrp_info(sig, __si_special(priv), pid);\n  read_unlock(&tasklist_lock);\n\n  return ret;\n}\n\nEXPORT_SYMBOL(kill_pgrp);\n\nint kill_pid(struct pid *pid, int sig, int priv) {\n  return kill_pid_info(sig, __si_special(priv), pid);\n}\n\nEXPORT_SYMBOL(kill_pid);\n\nstruct sigqueue *sigqueue_alloc(void) {\n  return __sigqueue_alloc(-1, current, GFP_KERNEL, 0, SIGQUEUE_PREALLOC);\n}\n\nvoid sigqueue_free(struct sigqueue *q) {\n  unsigned long flags;\n  spinlock_t *lock = &current->sighand->siglock;\n\n  BUG_ON(!(q->flags & SIGQUEUE_PREALLOC));\n\n  spin_lock_irqsave(lock, flags);\n  q->flags &= ~SIGQUEUE_PREALLOC;\n\n  if (!list_empty(&q->list))\n    q = NULL;\n  spin_unlock_irqrestore(lock, flags);\n\n  if (q)\n    __sigqueue_free(q);\n}\n\nint send_sigqueue(struct sigqueue *q, struct pid *pid, enum pid_type type) {\n  int sig = q->info.si_signo;\n  struct sigpending *pending;\n  struct task_struct *t;\n  unsigned long flags;\n  int ret, result;\n\n  BUG_ON(!(q->flags & SIGQUEUE_PREALLOC));\n\n  ret = -1;\n  rcu_read_lock();\n  t = pid_task(pid, type);\n  if (!t || !likely(lock_task_sighand(t, &flags)))\n    goto ret;\n\n  ret = 1;\n  result = TRACE_SIGNAL_IGNORED;\n  if (!prepare_signal(sig, t, false))\n    goto out;\n\n  ret = 0;\n  if (unlikely(!list_empty(&q->list))) {\n    BUG_ON(q->info.si_code != SI_TIMER);\n    q->info.si_overrun++;\n    result = TRACE_SIGNAL_ALREADY_PENDING;\n    goto out;\n  }\n  q->info.si_overrun = 0;\n\n  signalfd_notify(t, sig);\n  pending = (type != PIDTYPE_PID) ? &t->signal->shared_pending : &t->pending;\n  list_add_tail(&q->list, &pending->list);\n  sigaddset(&pending->signal, sig);\n  complete_signal(sig, t, type);\n  result = TRACE_SIGNAL_DELIVERED;\n  out:\n  trace_signal_generate(sig, &q->info, t, type != PIDTYPE_PID, result);\n  unlock_task_sighand(t, &flags);\n  ret:\n  rcu_read_unlock();\n  return ret;\n}\n\nstatic void do_notify_pidfd(struct task_struct *task) {\n  struct pid *pid;\n\n  WARN_ON(task->exit_state == 0);\n  pid = task_pid(task);\n  wake_up_all(&pid->wait_pidfd);\n}\n\nbool do_notify_parent(struct task_struct *tsk, int sig) {\n  struct kernel_siginfo info;\n  unsigned long flags;\n  struct sighand_struct *psig;\n  bool autoreap = false;\n  u64 utime, stime;\n\n  BUG_ON(sig == -1);\n\n  BUG_ON(task_is_stopped_or_traced(tsk));\n\n  BUG_ON(!tsk->ptrace &&\n         (tsk->group_leader != tsk || !thread_group_empty(tsk)));\n\n  do_notify_pidfd(tsk);\n\n  if (sig != SIGCHLD) {\n\n    if (tsk->parent_exec_id != READ_ONCE(tsk->parent->self_exec_id))\n      sig = SIGCHLD;\n  }\n\n  clear_siginfo(&info);\n  info.si_signo = sig;\n  info.si_errno = 0;\n  rcu_read_lock();\n  info.si_pid = task_pid_nr_ns(tsk, task_active_pid_ns(tsk->parent));\n  info.si_uid = from_kuid_munged(task_cred_xxx(tsk->parent, user_ns),\n                                 task_uid(tsk));\n  rcu_read_unlock();\n\n  task_cputime(tsk, &utime, &stime);\n  info.si_utime = nsec_to_clock_t(utime + tsk->signal->utime);\n  info.si_stime = nsec_to_clock_t(stime + tsk->signal->stime);\n\n  info.si_status = tsk->exit_code & 0x7f;\n  if (tsk->exit_code & 0x80)\n    info.si_code = CLD_DUMPED;\n  else if (tsk->exit_code & 0x7f)\n    info.si_code = CLD_KILLED;\n  else {\n    info.si_code = CLD_EXITED;\n    info.si_status = tsk->exit_code >> 8;\n  }\n\n  psig = tsk->parent->sighand;\n  spin_lock_irqsave(&psig->siglock, flags);\n  if (!tsk->ptrace && sig == SIGCHLD &&\n      (psig->action[SIGCHLD - 1].sa.sa_handler == SIG_IGN ||\n       (psig->action[SIGCHLD - 1].sa.sa_flags & SA_NOCLDWAIT))) {\n    autoreap = true;\n    if (psig->action[SIGCHLD - 1].sa.sa_handler == SIG_IGN)\n      sig = 0;\n  }\n  if (valid_signal(sig) && sig)\n    __send_signal(sig, &info, tsk->parent, PIDTYPE_TGID, false);\n  __wake_up_parent(tsk, tsk->parent);\n  spin_unlock_irqrestore(&psig->siglock, flags);\n\n  return autoreap;\n}\n\nstatic void do_notify_parent_cldstop(struct task_struct *tsk,\n                                     bool for_ptracer, int why) {\n  struct kernel_siginfo info;\n  unsigned long flags;\n  struct task_struct *parent;\n  struct sighand_struct *sighand;\n  u64 utime, stime;\n\n  if (for_ptracer) {\n    parent = tsk->parent;\n  } else {\n    tsk = tsk->group_leader;\n    parent = tsk->real_parent;\n  }\n\n  clear_siginfo(&info);\n  info.si_signo = SIGCHLD;\n  info.si_errno = 0;\n\n  rcu_read_lock();\n  info.si_pid = task_pid_nr_ns(tsk, task_active_pid_ns(parent));\n  info.si_uid = from_kuid_munged(task_cred_xxx(parent, user_ns), task_uid(tsk));\n  rcu_read_unlock();\n\n  task_cputime(tsk, &utime, &stime);\n  info.si_utime = nsec_to_clock_t(utime);\n  info.si_stime = nsec_to_clock_t(stime);\n\n  info.si_code = why;\n  switch (why) {\n    case CLD_CONTINUED:\n      info.si_status = SIGCONT;\n      break;\n    case CLD_STOPPED:\n      info.si_status = tsk->signal->group_exit_code & 0x7f;\n      break;\n    case CLD_TRAPPED:\n      info.si_status = tsk->exit_code & 0x7f;\n      break;\n    default:\n      BUG();\n  }\n\n  sighand = parent->sighand;\n  spin_lock_irqsave(&sighand->siglock, flags);\n  if (sighand->action[SIGCHLD - 1].sa.sa_handler != SIG_IGN &&\n      !(sighand->action[SIGCHLD - 1].sa.sa_flags & SA_NOCLDSTOP))\n    __group_send_sig_info(SIGCHLD, &info, parent);\n\n  __wake_up_parent(tsk, parent);\n  spin_unlock_irqrestore(&sighand->siglock, flags);\n}\n\nstatic void\nptrace_stop(int exit_code, int why, int clear_code, kernel_siginfo_t *info)\n\n__releases(&current\n->sighand->siglock)\n__acquires(&current\n->sighand->siglock)\n{\nbool gstop_done = false;\n\nif (\n\narch_ptrace_stop_needed()\n\n) {\n\nspin_unlock_irq(&current\n->sighand->siglock);\n\narch_ptrace_stop();\n\nspin_lock_irq(&current\n->sighand->siglock);\n}\nset_special_state(TASK_TRACED);\n\nsmp_wmb();\n\ncurrent->\nlast_siginfo = info;\ncurrent->\nexit_code = exit_code;\n\nif (why ==\nCLD_STOPPED &&(current\n->\njobctl &JOBCTL_STOP_PENDING\n))\ngstop_done = task_participate_group_stop(current);\n\ntask_clear_jobctl_pending(current, JOBCTL_TRAP_STOP\n);\nif (\ninfo &&info\n->si_code >> 8 == PTRACE_EVENT_STOP)\ntask_clear_jobctl_pending(current, JOBCTL_TRAP_NOTIFY\n);\n\ntask_clear_jobctl_trapping(current);\n\nspin_unlock_irq(&current\n->sighand->siglock);\nread_lock(&tasklist_lock);\nif (\nlikely(current\n->ptrace)) {\n\ndo_notify_parent_cldstop(current,\ntrue, why);\nif (\ngstop_done &&ptrace_reparented(current)\n)\ndo_notify_parent_cldstop(current,\nfalse, why);\n\npreempt_disable();\n\nread_unlock(&tasklist_lock);\n\ncgroup_enter_frozen();\n\npreempt_enable_no_resched();\n\nfreezable_schedule();\n\ncgroup_leave_frozen(true);\n} else {\n\nif (gstop_done)\ndo_notify_parent_cldstop(current,\nfalse, why);\n\n__set_current_state(TASK_RUNNING);\nif (clear_code)\ncurrent->\nexit_code = 0;\nread_unlock(&tasklist_lock);\n}\n\nspin_lock_irq(&current\n->sighand->siglock);\ncurrent->\nlast_siginfo = NULL;\n\ncurrent->jobctl &= ~\nJOBCTL_LISTENING;\n\nrecalc_sigpending_tsk(current);\n}\n\nstatic void ptrace_do_notify(int signr, int exit_code, int why) {\n  kernel_siginfo_t info;\n\n  clear_siginfo(&info);\n  info.si_signo = signr;\n  info.si_code = exit_code;\n  info.si_pid = task_pid_vnr(current);\n  info.si_uid = from_kuid_munged(current_user_ns(), current_uid());\n\n  ptrace_stop(exit_code, why, 1, &info);\n}\n\nvoid ptrace_notify(int exit_code) {\n  BUG_ON((exit_code & (0x7f | ~0xffff)) != SIGTRAP);\n  if (unlikely(current->task_works))\n    task_work_run();\n\n  spin_lock_irq(&current->sighand->siglock);\n  ptrace_do_notify(SIGTRAP, exit_code, CLD_TRAPPED);\n  spin_unlock_irq(&current->sighand->siglock);\n}\n\nstatic bool do_signal_stop(int signr)\n\n__releases(&current\n->sighand->siglock)\n{\nstruct signal_struct *sig = current->signal;\n\nif (!(current->\njobctl &JOBCTL_STOP_PENDING\n)) {\nunsigned long gstop = JOBCTL_STOP_PENDING | JOBCTL_STOP_CONSUME;\nstruct task_struct *t;\n\nWARN_ON_ONCE(signr\n& ~JOBCTL_STOP_SIGMASK);\n\nif (!\nlikely(current\n->\njobctl &JOBCTL_STOP_DEQUEUED\n) ||\nunlikely(sig\n->\nflags &SIGNAL_GROUP_EXIT\n) ||\nunlikely(sig\n->group_exec_task))\nreturn false;\n\nif (!(sig->\nflags &SIGNAL_STOP_STOPPED\n))\nsig->\ngroup_exit_code = signr;\n\nsig->\ngroup_stop_count = 0;\n\nif (\ntask_set_jobctl_pending(current, signr\n| gstop))\nsig->group_stop_count++;\n\nt = current;\nwhile_each_thread(current, t\n) {\n\nif (!\ntask_is_stopped(t)\n&&\ntask_set_jobctl_pending(t, signr\n| gstop)) {\nsig->group_stop_count++;\nif (likely(!(t->\nptrace &PT_SEIZED\n)))\nsignal_wake_up(t,\n0);\nelse\nptrace_trap_notify(t);\n}\n}\n}\n\nif (likely(!current->ptrace)) {\nint notify = 0;\n\nif (\ntask_participate_group_stop(current)\n)\nnotify = CLD_STOPPED;\n\nset_special_state(TASK_STOPPED);\nspin_unlock_irq(&current\n->sighand->siglock);\n\n\nif (notify) {\nread_lock(&tasklist_lock);\ndo_notify_parent_cldstop(current,\nfalse, notify);\nread_unlock(&tasklist_lock);\n}\n\ncgroup_enter_frozen();\n\nfreezable_schedule();\n\nreturn true;\n} else {\n\ntask_set_jobctl_pending(current, JOBCTL_TRAP_STOP\n);\nreturn false;\n}\n}\n\nstatic void do_jobctl_trap(void) {\n  struct signal_struct *signal = current->signal;\n  int signr = current->jobctl & JOBCTL_STOP_SIGMASK;\n\n  if (current->ptrace & PT_SEIZED) {\n    if (!signal->group_stop_count &&\n        !(signal->flags & SIGNAL_STOP_STOPPED))\n      signr = SIGTRAP;\n    WARN_ON_ONCE(!signr);\n    ptrace_do_notify(signr, signr | (PTRACE_EVENT_STOP << 8),\n                     CLD_STOPPED);\n  } else {\n    WARN_ON_ONCE(!signr);\n    ptrace_stop(signr, CLD_STOPPED, 0, NULL);\n    current->exit_code = 0;\n  }\n}\n\nstatic void do_freezer_trap(void)\n\n__releases(&current\n->sighand->siglock)\n{\n\nif ((current->\njobctl &(JOBCTL_PENDING_MASK\n| JOBCTL_TRAP_FREEZE)) !=\nJOBCTL_TRAP_FREEZE) {\nspin_unlock_irq(&current\n->sighand->siglock);\nreturn;\n}\n\n__set_current_state(TASK_INTERRUPTIBLE);\nclear_thread_flag(TIF_SIGPENDING);\nspin_unlock_irq(&current\n->sighand->siglock);\n\ncgroup_enter_frozen();\n\nfreezable_schedule();\n\n}\n\nstatic int\nptrace_signal(int signr, kernel_siginfo_t *info, enum pid_type type) {\n\n  current->jobctl |= JOBCTL_STOP_DEQUEUED;\n  ptrace_stop(signr, CLD_TRAPPED, 0, info);\n\n  signr = current->exit_code;\n  if (signr == 0)\n    return signr;\n\n  current->exit_code = 0;\n\n  if (signr != info->si_signo) {\n    clear_siginfo(info);\n    info->si_signo = signr;\n    info->si_errno = 0;\n    info->si_code = SI_USER;\n    rcu_read_lock();\n    info->si_pid = task_pid_vnr(current->parent);\n    info->si_uid = from_kuid_munged(current_user_ns(),\n                                    task_uid(current->parent));\n    rcu_read_unlock();\n  }\n\n  if (sigismember(&current->blocked, signr) ||\n      fatal_signal_pending(current)) {\n    send_signal(signr, info, current, type);\n    signr = 0;\n  }\n\n  return signr;\n}\n\nstatic void hide_si_addr_tag_bits(struct ksignal *ksig) {\n  switch (siginfo_layout(ksig->sig, ksig->info.si_code)) {\n    case SIL_FAULT:\n    case SIL_FAULT_TRAPNO:\n    case SIL_FAULT_MCEERR:\n    case SIL_FAULT_BNDERR:\n    case SIL_FAULT_PKUERR:\n    case SIL_FAULT_PERF_EVENT:\n      ksig->info.si_addr = arch_untagged_si_addr(\n          ksig->info.si_addr, ksig->sig, ksig->info.si_code);\n      break;\n    case SIL_KILL:\n    case SIL_TIMER:\n    case SIL_POLL:\n    case SIL_CHLD:\n    case SIL_RT:\n    case SIL_SYS:\n      break;\n  }\n}\n\nbool get_signal(struct ksignal *ksig) {\n  struct sighand_struct *sighand = current->sighand;\n  struct signal_struct *signal = current->signal;\n  int signr;\n\n  if (unlikely(current->task_works))\n    task_work_run();\n\n  if (!IS_ENABLED(CONFIG_GENERIC_ENTRY)) {\n    if (test_thread_flag(TIF_NOTIFY_SIGNAL))\n      tracehook_notify_signal();\n    if (!task_sigpending(current))\n      return false;\n  }\n\n  if (unlikely(uprobe_deny_signal()))\n    return false;\n\n  try_to_freeze();\n\n  relock:\n  spin_lock_irq(&sighand->siglock);\n\n  if (unlikely(signal->flags & SIGNAL_CLD_MASK)) {\n    int why;\n\n    if (signal->flags & SIGNAL_CLD_CONTINUED)\n      why = CLD_CONTINUED;\n    else\n      why = CLD_STOPPED;\n\n    signal->flags &= ~SIGNAL_CLD_MASK;\n\n    spin_unlock_irq(&sighand->siglock);\n\n    read_lock(&tasklist_lock);\n    do_notify_parent_cldstop(current, false, why);\n\n    if (ptrace_reparented(current->group_leader))\n      do_notify_parent_cldstop(current->group_leader,\n                               true, why);\n    read_unlock(&tasklist_lock);\n\n    goto relock;\n  }\n\n  for (;;) {\n    struct k_sigaction *ka;\n    enum pid_type type;\n\n    if ((signal->flags & SIGNAL_GROUP_EXIT) ||\n        signal->group_exec_task) {\n      ksig->info.si_signo = signr = SIGKILL;\n      sigdelset(&current->pending.signal, SIGKILL);\n      trace_signal_deliver(SIGKILL, SEND_SIG_NOINFO,\n                           &sighand->action[SIGKILL - 1]);\n      recalc_sigpending();\n      goto fatal;\n    }\n\n    if (unlikely(current->jobctl & JOBCTL_STOP_PENDING) &&\n        do_signal_stop(0))\n      goto relock;\n\n    if (unlikely(current->jobctl &\n                 (JOBCTL_TRAP_MASK | JOBCTL_TRAP_FREEZE))) {\n      if (current->jobctl & JOBCTL_TRAP_MASK) {\n        do_jobctl_trap();\n        spin_unlock_irq(&sighand->siglock);\n      } else if (current->jobctl & JOBCTL_TRAP_FREEZE)\n        do_freezer_trap();\n\n      goto relock;\n    }\n\n    if (unlikely(cgroup_task_frozen(current))) {\n      spin_unlock_irq(&sighand->siglock);\n      cgroup_leave_frozen(false);\n      goto relock;\n    }\n\n    type = PIDTYPE_PID;\n    signr = dequeue_synchronous_signal(&ksig->info);\n    if (!signr)\n      signr = dequeue_signal(current, &current->blocked,\n                             &ksig->info, &type);\n\n    if (!signr)\n      break;\n\n    if (unlikely(current->ptrace) && (signr != SIGKILL) &&\n        !(sighand->action[signr - 1].sa.sa_flags & SA_IMMUTABLE)) {\n      signr = ptrace_signal(signr, &ksig->info, type);\n      if (!signr)\n        continue;\n    }\n\n    ka = &sighand->action[signr - 1];\n\n    trace_signal_deliver(signr, &ksig->info, ka);\n\n    if (ka->sa.sa_handler == SIG_IGN)\n      continue;\n    if (ka->sa.sa_handler != SIG_DFL) {\n\n      ksig->ka = *ka;\n\n      if (ka->sa.sa_flags & SA_ONESHOT)\n        ka->sa.sa_handler = SIG_DFL;\n\n      break;\n    }\n\n    if (sig_kernel_ignore(signr))\n      continue;\n\n    if (unlikely(signal->flags & SIGNAL_UNKILLABLE) &&\n        !sig_kernel_only(signr))\n      continue;\n\n    if (sig_kernel_stop(signr)) {\n\n      if (signr != SIGSTOP) {\n        spin_unlock_irq(&sighand->siglock);\n\n        if (is_current_pgrp_orphaned())\n          goto relock;\n\n        spin_lock_irq(&sighand->siglock);\n      }\n\n      if (likely(do_signal_stop(ksig->info.si_signo))) {\n\n        goto relock;\n      }\n\n      continue;\n    }\n\n    fatal:\n    spin_unlock_irq(&sighand->siglock);\n    if (unlikely(cgroup_task_frozen(current)))\n      cgroup_leave_frozen(true);\n\n    current->flags |= PF_SIGNALED;\n\n    if (sig_kernel_coredump(signr)) {\n      if (print_fatal_signals)\n        print_fatal_signal(ksig->info.si_signo);\n      proc_coredump_connector(current);\n\n      do_coredump(&ksig->info);\n    }\n\n    if (current->flags & PF_IO_WORKER)\n      goto out;\n\n    do_group_exit(ksig->info.si_signo);\n\n  }\n  spin_unlock_irq(&sighand->siglock);\n  out:\n  ksig->sig = signr;\n\n  if (!(ksig->ka.sa.sa_flags & SA_EXPOSE_TAGBITS))\n    hide_si_addr_tag_bits(ksig);\n\n  return ksig->sig > 0;\n}\n\nstatic void signal_delivered(struct ksignal *ksig, int stepping) {\n  sigset_t blocked;\n\n  clear_restore_sigmask();\n\n  sigorsets(&blocked, &current->blocked, &ksig->ka.sa.sa_mask);\n  if (!(ksig->ka.sa.sa_flags & SA_NODEFER))\n    sigaddset(&blocked, ksig->sig);\n  set_current_blocked(&blocked);\n  if (current->sas_ss_flags & SS_AUTODISARM)\n    sas_ss_reset(current);\n  tracehook_signal_handler(stepping);\n}\n\nvoid signal_setup_done(int failed, struct ksignal *ksig, int stepping) {\n  if (failed)\n    force_sigsegv(ksig->sig);\n  else\n    signal_delivered(ksig, stepping);\n}\n\nstatic void retarget_shared_pending(struct task_struct *tsk, sigset_t *which) {\n  sigset_t retarget;\n  struct task_struct *t;\n\n  sigandsets(&retarget, &tsk->signal->shared_pending.signal, which);\n  if (sigisemptyset(&retarget))\n    return;\n\n  t = tsk;\n  while_each_thread(tsk, t)\n  {\n    if (t->flags & PF_EXITING)\n      continue;\n\n    if (!has_pending_signals(&retarget, &t->blocked))\n      continue;\n\n    sigandsets(&retarget, &retarget, &t->blocked);\n\n    if (!task_sigpending(t))\n      signal_wake_up(t, 0);\n\n    if (sigisemptyset(&retarget))\n      break;\n  }\n}\n\nvoid exit_signals(struct task_struct *tsk) {\n  int group_stop = 0;\n  sigset_t unblocked;\n\n  cgroup_threadgroup_change_begin(tsk);\n\n  if (thread_group_empty(tsk) || (tsk->signal->flags & SIGNAL_GROUP_EXIT)) {\n    tsk->flags |= PF_EXITING;\n    cgroup_threadgroup_change_end(tsk);\n    return;\n  }\n\n  spin_lock_irq(&tsk->sighand->siglock);\n\n  tsk->flags |= PF_EXITING;\n\n  cgroup_threadgroup_change_end(tsk);\n\n  if (!task_sigpending(tsk))\n    goto out;\n\n  unblocked = tsk->blocked;\n  signotset(&unblocked);\n  retarget_shared_pending(tsk, &unblocked);\n\n  if (unlikely(tsk->jobctl & JOBCTL_STOP_PENDING) &&\n      task_participate_group_stop(tsk))\n    group_stop = CLD_STOPPED;\n  out:\n  spin_unlock_irq(&tsk->sighand->siglock);\n\n  if (unlikely(group_stop)) {\n    read_lock(&tasklist_lock);\n    do_notify_parent_cldstop(tsk, false, group_stop);\n    read_unlock(&tasklist_lock);\n  }\n}\n\nSYSCALL_DEFINE0(restart_syscall)\n    {\n        struct restart_block *restart = &current->restart_block;\n        return restart->fn(restart);\n    }\n\nlong do_no_restart_syscall(struct restart_block *param) {\n  return -EINTR;\n}\n\nstatic void\n__set_task_blocked(struct task_struct *tsk, const sigset_t *newset) {\n  if (task_sigpending(tsk) && !thread_group_empty(tsk)) {\n    sigset_t newblocked;\n\n    sigandnsets(&newblocked, newset, &current->blocked);\n    retarget_shared_pending(tsk, &newblocked);\n  }\n  tsk->blocked = *newset;\n  recalc_sigpending();\n}\n\nvoid set_current_blocked(sigset_t *newset) {\n  sigdelsetmask(newset, sigmask(SIGKILL) | sigmask(SIGSTOP));\n  __set_current_blocked(newset);\n}\n\nvoid __set_current_blocked(const sigset_t *newset) {\n  struct task_struct *tsk = current;\n\n  if (sigequalsets(&tsk->blocked, newset))\n    return;\n\n  spin_lock_irq(&tsk->sighand->siglock);\n  __set_task_blocked(tsk, newset);\n  spin_unlock_irq(&tsk->sighand->siglock);\n}\n\nint sigprocmask(int how, sigset_t *set, sigset_t *oldset) {\n  struct task_struct *tsk = current;\n  sigset_t newset;\n\n  if (oldset)\n    *oldset = tsk->blocked;\n\n  switch (how) {\n    case SIG_BLOCK:\n      sigorsets(&newset, &tsk->blocked, set);\n      break;\n    case SIG_UNBLOCK:\n      sigandnsets(&newset, &tsk->blocked, set);\n      break;\n    case SIG_SETMASK:\n      newset = *set;\n      break;\n    default:\n      return -EINVAL;\n  }\n\n  __set_current_blocked(&newset);\n  return 0;\n}\n\nEXPORT_SYMBOL(sigprocmask);\n\nint set_user_sigmask(const sigset_t __user\n\n*umask,\nsize_t sigsetsize\n)\n{\nsigset_t kmask;\n\nif (!umask)\nreturn 0;\nif (sigsetsize != sizeof(sigset_t))\nreturn -\nEINVAL;\nif (\ncopy_from_user(&kmask, umask,\nsizeof(sigset_t)))\nreturn -\nEFAULT;\n\nset_restore_sigmask();\n\ncurrent->\nsaved_sigmask = current->blocked;\nset_current_blocked(&kmask);\n\nreturn 0;\n}\n\n#ifdef CONFIG_COMPAT\nint set_compat_user_sigmask(const compat_sigset_t __user *umask,\n          size_t sigsetsize)\n{\n  sigset_t kmask;\n\n  if (!umask)\n    return 0;\n  if (sigsetsize != sizeof(compat_sigset_t))\n    return -EINVAL;\n  if (get_compat_sigset(&kmask, umask))\n    return -EFAULT;\n\n  set_restore_sigmask();\n  current->saved_sigmask = current->blocked;\n  set_current_blocked(&kmask);\n\n  return 0;\n}\n#endif\n\nSYSCALL_DEFINE4(rt_sigprocmask,\nint, how,\nsigset_t __user\n*, nset,\nsigset_t __user\n*, oset, size_t, sigsetsize)\n{\nsigset_t old_set, new_set;\nint error;\n\nif (sigsetsize != sizeof(sigset_t))\nreturn -\nEINVAL;\n\nold_set = current->blocked;\n\nif (nset) {\nif (\ncopy_from_user(&new_set, nset,\nsizeof(sigset_t)))\nreturn -\nEFAULT;\nsigdelsetmask(&new_set, sigmask(SIGKILL)\n|\nsigmask(SIGSTOP)\n);\n\nerror = sigprocmask(how, &new_set, NULL);\nif (error)\nreturn\nerror;\n}\n\nif (oset) {\nif (\ncopy_to_user(oset, &old_set,\nsizeof(sigset_t)))\nreturn -\nEFAULT;\n}\n\nreturn 0;\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE4(rt_sigprocmask, int, how, compat_sigset_t __user *, nset,\n    compat_sigset_t __user *, oset, compat_size_t, sigsetsize)\n{\n  sigset_t old_set = current->blocked;\n\n\n  if (sigsetsize != sizeof(sigset_t))\n    return -EINVAL;\n\n  if (nset) {\n    sigset_t new_set;\n    int error;\n    if (get_compat_sigset(&new_set, nset))\n      return -EFAULT;\n    sigdelsetmask(&new_set, sigmask(SIGKILL)|sigmask(SIGSTOP));\n\n    error = sigprocmask(how, &new_set, NULL);\n    if (error)\n      return error;\n  }\n  return oset ? put_compat_sigset(oset, &old_set, sizeof(*oset)) : 0;\n}\n#endif\n\nstatic void do_sigpending(sigset_t *set) {\n  spin_lock_irq(&current->sighand->siglock);\n  sigorsets(set, &current->pending.signal,\n            &current->signal->shared_pending.signal);\n  spin_unlock_irq(&current->sighand->siglock);\n\n  sigandsets(set, &current->blocked, set);\n}\n\nSYSCALL_DEFINE2(rt_sigpending, sigset_t\n__user *, uset, size_t, sigsetsize)\n{\nsigset_t set;\n\nif (sigsetsize > sizeof(*uset))\nreturn -\nEINVAL;\n\ndo_sigpending(&set);\n\nif (\ncopy_to_user(uset, &set, sigsetsize\n))\nreturn -\nEFAULT;\n\nreturn 0;\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE2(rt_sigpending, compat_sigset_t __user *, uset,\n    compat_size_t, sigsetsize)\n{\n  sigset_t set;\n\n  if (sigsetsize > sizeof(*uset))\n    return -EINVAL;\n\n  do_sigpending(&set);\n\n  return put_compat_sigset(uset, &set, sigsetsize);\n}\n#endif\n\nstatic const struct {\n  unsigned char limit, layout;\n} sig_sicodes[] = {\n    [SIGILL] = {NSIGILL, SIL_FAULT},\n    [SIGFPE] = {NSIGFPE, SIL_FAULT},\n    [SIGSEGV] = {NSIGSEGV, SIL_FAULT},\n    [SIGBUS] = {NSIGBUS, SIL_FAULT},\n    [SIGTRAP] = {NSIGTRAP, SIL_FAULT},\n#if defined(SIGEMT)\n    [SIGEMT] = { NSIGEMT, SIL_FAULT },\n#endif\n    [SIGCHLD] = {NSIGCHLD, SIL_CHLD},\n    [SIGPOLL] = {NSIGPOLL, SIL_POLL},\n    [SIGSYS] = {NSIGSYS, SIL_SYS},\n};\n\nstatic bool known_siginfo_layout(unsigned sig, int si_code) {\n  if (si_code == SI_KERNEL)\n    return true;\n  else if ((si_code > SI_USER)) {\n    if (sig_specific_sicodes(sig)) {\n      if (si_code <= sig_sicodes[sig].limit)\n        return true;\n    } else if (si_code <= NSIGPOLL)\n      return true;\n  } else if (si_code >= SI_DETHREAD)\n    return true;\n  else if (si_code == SI_ASYNCNL)\n    return true;\n  return false;\n}\n\nenum siginfo_layout siginfo_layout(unsigned sig, int si_code) {\n  enum siginfo_layout layout = SIL_KILL;\n  if ((si_code > SI_USER) && (si_code < SI_KERNEL)) {\n    if ((sig < ARRAY_SIZE(sig_sicodes)) &&\n        (si_code <= sig_sicodes[sig].limit)) {\n      layout = sig_sicodes[sig].layout;\n\n      if ((sig == SIGBUS) &&\n          (si_code >= BUS_MCEERR_AR) && (si_code <= BUS_MCEERR_AO))\n        layout = SIL_FAULT_MCEERR;\n      else if ((sig == SIGSEGV) && (si_code == SEGV_BNDERR))\n        layout = SIL_FAULT_BNDERR;\n#ifdef SEGV_PKUERR\n        else if ((sig == SIGSEGV) && (si_code == SEGV_PKUERR))\n          layout = SIL_FAULT_PKUERR;\n#endif\n      else if ((sig == SIGTRAP) && (si_code == TRAP_PERF))\n        layout = SIL_FAULT_PERF_EVENT;\n      else if (IS_ENABLED(CONFIG_SPARC) &&\n               (sig == SIGILL) && (si_code == ILL_ILLTRP))\n        layout = SIL_FAULT_TRAPNO;\n      else if (IS_ENABLED(CONFIG_ALPHA) &&\n               ((sig == SIGFPE) ||\n                ((sig == SIGTRAP) && (si_code == TRAP_UNK))))\n        layout = SIL_FAULT_TRAPNO;\n    } else if (si_code <= NSIGPOLL)\n      layout = SIL_POLL;\n  } else {\n    if (si_code == SI_TIMER)\n      layout = SIL_TIMER;\n    else if (si_code == SI_SIGIO)\n      layout = SIL_POLL;\n    else if (si_code < 0)\n      layout = SIL_RT;\n  }\n  return layout;\n}\n\nstatic inline char __user\n*\n\nsi_expansion(const siginfo_t __user\n\n*info)\n{\nreturn ((\nchar __user\n*)info) + sizeof(struct kernel_siginfo);\n}\n\nint copy_siginfo_to_user(siginfo_t __user\n\n*to,\nconst kernel_siginfo_t *from\n)\n{\nchar __user\n*\nexpansion = si_expansion(to);\nif (\ncopy_to_user(to, from,\nsizeof(struct kernel_siginfo)))\nreturn -\nEFAULT;\nif (\nclear_user(expansion, SI_EXPANSION_SIZE\n))\nreturn -\nEFAULT;\nreturn 0;\n}\n\nstatic int post_copy_siginfo_from_user(kernel_siginfo_t *info,\n                                       const siginfo_t __user\n\n*from)\n{\nif (unlikely(!\nknown_siginfo_layout(info\n->si_signo, info->si_code))) {\nchar __user\n*\nexpansion = si_expansion(from);\nchar buf[SI_EXPANSION_SIZE];\nint i;\n\nif (\ncopy_from_user(&buf, expansion, SI_EXPANSION_SIZE\n))\nreturn -\nEFAULT;\nfor (\ni = 0;\ni<SI_EXPANSION_SIZE;\ni++) {\nif (buf[i] != 0)\nreturn -\nE2BIG;\n}\n}\nreturn 0;\n}\n\nstatic int __copy_siginfo_from_user(int signo, kernel_siginfo_t *to,\n                                    const siginfo_t __user\n\n*from)\n{\nif (\ncopy_from_user(to, from,\nsizeof(struct kernel_siginfo)))\nreturn -\nEFAULT;\nto->\nsi_signo = signo;\nreturn\npost_copy_siginfo_from_user(to, from\n);\n}\n\nint copy_siginfo_from_user(kernel_siginfo_t *to, const siginfo_t __user\n\n*from)\n{\nif (\ncopy_from_user(to, from,\nsizeof(struct kernel_siginfo)))\nreturn -\nEFAULT;\nreturn\npost_copy_siginfo_from_user(to, from\n);\n}\n\n#ifdef CONFIG_COMPAT\n\nvoid copy_siginfo_to_external32(struct compat_siginfo *to,\n    const struct kernel_siginfo *from)\n{\n  memset(to, 0, sizeof(*to));\n\n  to->si_signo = from->si_signo;\n  to->si_errno = from->si_errno;\n  to->si_code = from->si_code;\n  switch(siginfo_layout(from->si_signo, from->si_code)) {\n  case SIL_KILL:\n    to->si_pid = from->si_pid;\n    to->si_uid = from->si_uid;\n    break;\n  case SIL_TIMER:\n    to->si_tid = from->si_tid;\n    to->si_overrun = from->si_overrun;\n    to->si_int = from->si_int;\n    break;\n  case SIL_POLL:\n    to->si_band = from->si_band;\n    to->si_fd = from->si_fd;\n    break;\n  case SIL_FAULT:\n    to->si_addr = ptr_to_compat(from->si_addr);\n    break;\n  case SIL_FAULT_TRAPNO:\n    to->si_addr = ptr_to_compat(from->si_addr);\n    to->si_trapno = from->si_trapno;\n    break;\n  case SIL_FAULT_MCEERR:\n    to->si_addr = ptr_to_compat(from->si_addr);\n    to->si_addr_lsb = from->si_addr_lsb;\n    break;\n  case SIL_FAULT_BNDERR:\n    to->si_addr = ptr_to_compat(from->si_addr);\n    to->si_lower = ptr_to_compat(from->si_lower);\n    to->si_upper = ptr_to_compat(from->si_upper);\n    break;\n  case SIL_FAULT_PKUERR:\n    to->si_addr = ptr_to_compat(from->si_addr);\n    to->si_pkey = from->si_pkey;\n    break;\n  case SIL_FAULT_PERF_EVENT:\n    to->si_addr = ptr_to_compat(from->si_addr);\n    to->si_perf_data = from->si_perf_data;\n    to->si_perf_type = from->si_perf_type;\n    break;\n  case SIL_CHLD:\n    to->si_pid = from->si_pid;\n    to->si_uid = from->si_uid;\n    to->si_status = from->si_status;\n    to->si_utime = from->si_utime;\n    to->si_stime = from->si_stime;\n    break;\n  case SIL_RT:\n    to->si_pid = from->si_pid;\n    to->si_uid = from->si_uid;\n    to->si_int = from->si_int;\n    break;\n  case SIL_SYS:\n    to->si_call_addr = ptr_to_compat(from->si_call_addr);\n    to->si_syscall = from->si_syscall;\n    to->si_arch = from->si_arch;\n    break;\n  }\n}\n\nint __copy_siginfo_to_user32(struct compat_siginfo __user *to,\n         const struct kernel_siginfo *from)\n{\n  struct compat_siginfo new;\n\n  copy_siginfo_to_external32(&new, from);\n  if (copy_to_user(to, &new, sizeof(struct compat_siginfo)))\n    return -EFAULT;\n  return 0;\n}\n\nstatic int post_copy_siginfo_from_user32(kernel_siginfo_t *to,\n           const struct compat_siginfo *from)\n{\n  clear_siginfo(to);\n  to->si_signo = from->si_signo;\n  to->si_errno = from->si_errno;\n  to->si_code = from->si_code;\n  switch(siginfo_layout(from->si_signo, from->si_code)) {\n  case SIL_KILL:\n    to->si_pid = from->si_pid;\n    to->si_uid = from->si_uid;\n    break;\n  case SIL_TIMER:\n    to->si_tid = from->si_tid;\n    to->si_overrun = from->si_overrun;\n    to->si_int = from->si_int;\n    break;\n  case SIL_POLL:\n    to->si_band = from->si_band;\n    to->si_fd = from->si_fd;\n    break;\n  case SIL_FAULT:\n    to->si_addr = compat_ptr(from->si_addr);\n    break;\n  case SIL_FAULT_TRAPNO:\n    to->si_addr = compat_ptr(from->si_addr);\n    to->si_trapno = from->si_trapno;\n    break;\n  case SIL_FAULT_MCEERR:\n    to->si_addr = compat_ptr(from->si_addr);\n    to->si_addr_lsb = from->si_addr_lsb;\n    break;\n  case SIL_FAULT_BNDERR:\n    to->si_addr = compat_ptr(from->si_addr);\n    to->si_lower = compat_ptr(from->si_lower);\n    to->si_upper = compat_ptr(from->si_upper);\n    break;\n  case SIL_FAULT_PKUERR:\n    to->si_addr = compat_ptr(from->si_addr);\n    to->si_pkey = from->si_pkey;\n    break;\n  case SIL_FAULT_PERF_EVENT:\n    to->si_addr = compat_ptr(from->si_addr);\n    to->si_perf_data = from->si_perf_data;\n    to->si_perf_type = from->si_perf_type;\n    break;\n  case SIL_CHLD:\n    to->si_pid = from->si_pid;\n    to->si_uid = from->si_uid;\n    to->si_status = from->si_status;\n#ifdef CONFIG_X86_X32_ABI\n    if (in_x32_syscall()) {\n      to->si_utime = from->_sifields._sigchld_x32._utime;\n      to->si_stime = from->_sifields._sigchld_x32._stime;\n    } else\n#endif\n    {\n      to->si_utime = from->si_utime;\n      to->si_stime = from->si_stime;\n    }\n    break;\n  case SIL_RT:\n    to->si_pid = from->si_pid;\n    to->si_uid = from->si_uid;\n    to->si_int = from->si_int;\n    break;\n  case SIL_SYS:\n    to->si_call_addr = compat_ptr(from->si_call_addr);\n    to->si_syscall = from->si_syscall;\n    to->si_arch = from->si_arch;\n    break;\n  }\n  return 0;\n}\n\nstatic int __copy_siginfo_from_user32(int signo, struct kernel_siginfo *to,\n              const struct compat_siginfo __user *ufrom)\n{\n  struct compat_siginfo from;\n\n  if (copy_from_user(&from, ufrom, sizeof(struct compat_siginfo)))\n    return -EFAULT;\n\n  from.si_signo = signo;\n  return post_copy_siginfo_from_user32(to, &from);\n}\n\nint copy_siginfo_from_user32(struct kernel_siginfo *to,\n           const struct compat_siginfo __user *ufrom)\n{\n  struct compat_siginfo from;\n\n  if (copy_from_user(&from, ufrom, sizeof(struct compat_siginfo)))\n    return -EFAULT;\n\n  return post_copy_siginfo_from_user32(to, &from);\n}\n#endif\n\nstatic int do_sigtimedwait(const sigset_t *which, kernel_siginfo_t *info,\n                           const struct timespec64 *ts) {\n  ktime_t *to = NULL, timeout = KTIME_MAX;\n  struct task_struct *tsk = current;\n  sigset_t mask = *which;\n  enum pid_type type;\n  int sig, ret = 0;\n\n  if (ts) {\n    if (!timespec64_valid(ts))\n      return -EINVAL;\n    timeout = timespec64_to_ktime(*ts);\n    to = &timeout;\n  }\n\n  sigdelsetmask(&mask, sigmask(SIGKILL) | sigmask(SIGSTOP));\n  signotset(&mask);\n\n  spin_lock_irq(&tsk->sighand->siglock);\n  sig = dequeue_signal(tsk, &mask, info, &type);\n  if (!sig && timeout) {\n\n    tsk->real_blocked = tsk->blocked;\n    sigandsets(&tsk->blocked, &tsk->blocked, &mask);\n    recalc_sigpending();\n    spin_unlock_irq(&tsk->sighand->siglock);\n\n    __set_current_state(TASK_INTERRUPTIBLE);\n    ret = freezable_schedule_hrtimeout_range(to, tsk->timer_slack_ns,\n                                             HRTIMER_MODE_REL);\n    spin_lock_irq(&tsk->sighand->siglock);\n    __set_task_blocked(tsk, &tsk->real_blocked);\n    sigemptyset(&tsk->real_blocked);\n    sig = dequeue_signal(tsk, &mask, info, &type);\n  }\n  spin_unlock_irq(&tsk->sighand->siglock);\n\n  if (sig)\n    return sig;\n  return ret ? -EINTR : -EAGAIN;\n}\n\nSYSCALL_DEFINE4(rt_sigtimedwait,\nconst sigset_t __user\n*, uthese,\nsiginfo_t __user\n*, uinfo,\nconst struct __kernel_timespec __user\n*, uts,\nsize_t, sigsetsize)\n{\nsigset_t these;\nstruct timespec64 ts;\nkernel_siginfo_t info;\nint ret;\n\nif (sigsetsize != sizeof(sigset_t))\nreturn -\nEINVAL;\n\nif (\ncopy_from_user(&these, uthese,\nsizeof(these)))\nreturn -\nEFAULT;\n\nif (uts) {\nif (\nget_timespec64(&ts, uts\n))\nreturn -\nEFAULT;\n}\n\nret = do_sigtimedwait(&these, &info, uts ? &ts : NULL);\n\nif (ret > 0 && uinfo) {\nif (\ncopy_siginfo_to_user(uinfo, &info\n))\nret = -EFAULT;\n}\n\nreturn\nret;\n}\n\n#ifdef CONFIG_COMPAT_32BIT_TIME\nSYSCALL_DEFINE4(rt_sigtimedwait_time32, const sigset_t __user *, uthese,\n    siginfo_t __user *, uinfo,\n    const struct old_timespec32 __user *, uts,\n    size_t, sigsetsize)\n{\n  sigset_t these;\n  struct timespec64 ts;\n  kernel_siginfo_t info;\n  int ret;\n\n  if (sigsetsize != sizeof(sigset_t))\n    return -EINVAL;\n\n  if (copy_from_user(&these, uthese, sizeof(these)))\n    return -EFAULT;\n\n  if (uts) {\n    if (get_old_timespec32(&ts, uts))\n      return -EFAULT;\n  }\n\n  ret = do_sigtimedwait(&these, &info, uts ? &ts : NULL);\n\n  if (ret > 0 && uinfo) {\n    if (copy_siginfo_to_user(uinfo, &info))\n      ret = -EFAULT;\n  }\n\n  return ret;\n}\n#endif\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE4(rt_sigtimedwait_time64, compat_sigset_t __user *, uthese,\n    struct compat_siginfo __user *, uinfo,\n    struct __kernel_timespec __user *, uts, compat_size_t, sigsetsize)\n{\n  sigset_t s;\n  struct timespec64 t;\n  kernel_siginfo_t info;\n  long ret;\n\n  if (sigsetsize != sizeof(sigset_t))\n    return -EINVAL;\n\n  if (get_compat_sigset(&s, uthese))\n    return -EFAULT;\n\n  if (uts) {\n    if (get_timespec64(&t, uts))\n      return -EFAULT;\n  }\n\n  ret = do_sigtimedwait(&s, &info, uts ? &t : NULL);\n\n  if (ret > 0 && uinfo) {\n    if (copy_siginfo_to_user32(uinfo, &info))\n      ret = -EFAULT;\n  }\n\n  return ret;\n}\n\n#ifdef CONFIG_COMPAT_32BIT_TIME\nCOMPAT_SYSCALL_DEFINE4(rt_sigtimedwait_time32, compat_sigset_t __user *, uthese,\n    struct compat_siginfo __user *, uinfo,\n    struct old_timespec32 __user *, uts, compat_size_t, sigsetsize)\n{\n  sigset_t s;\n  struct timespec64 t;\n  kernel_siginfo_t info;\n  long ret;\n\n  if (sigsetsize != sizeof(sigset_t))\n    return -EINVAL;\n\n  if (get_compat_sigset(&s, uthese))\n    return -EFAULT;\n\n  if (uts) {\n    if (get_old_timespec32(&t, uts))\n      return -EFAULT;\n  }\n\n  ret = do_sigtimedwait(&s, &info, uts ? &t : NULL);\n\n  if (ret > 0 && uinfo) {\n    if (copy_siginfo_to_user32(uinfo, &info))\n      ret = -EFAULT;\n  }\n\n  return ret;\n}\n#endif\n#endif\n\nstatic inline void prepare_kill_siginfo(int sig, struct kernel_siginfo *info) {\n  clear_siginfo(info);\n  info->si_signo = sig;\n  info->si_errno = 0;\n  info->si_code = SI_USER;\n  info->si_pid = task_tgid_vnr(current);\n  info->si_uid = from_kuid_munged(current_user_ns(), current_uid());\n}\n\nSYSCALL_DEFINE2(kill, pid_t, pid,\nint, sig)\n{\nstruct kernel_siginfo info;\n\nprepare_kill_siginfo(sig, &info\n);\n\nreturn\nkill_something_info(sig, &info, pid\n);\n}\n\nstatic bool access_pidfd_pidns(struct pid *pid) {\n  struct pid_namespace *active = task_active_pid_ns(current);\n  struct pid_namespace *p = ns_of_pid(pid);\n\n  for (;;) {\n    if (!p)\n      return false;\n    if (p == active)\n      break;\n    p = p->parent;\n  }\n\n  return true;\n}\n\nstatic int copy_siginfo_from_user_any(kernel_siginfo_t *kinfo,\n                                      siginfo_t __user\n\n*info)\n{\n#ifdef CONFIG_COMPAT\n\n\n\n\n\nif (in_compat_syscall())\n  return copy_siginfo_from_user32(\n    kinfo, (struct compat_siginfo __user *)info);\n#endif\nreturn\ncopy_siginfo_from_user(kinfo, info\n);\n}\n\nstatic struct pid *pidfd_to_pid(const struct file *file) {\n  struct pid *pid;\n\n  pid = pidfd_pid(file);\n  if (!IS_ERR(pid))\n    return pid;\n\n  return tgid_pidfd_to_pid(file);\n}\n\nSYSCALL_DEFINE4(pidfd_send_signal,\nint, pidfd, int, sig,\nsiginfo_t __user\n*, info, unsigned int, flags)\n{\nint ret;\nstruct fd f;\nstruct pid *pid;\nkernel_siginfo_t kinfo;\n\nif (flags)\nreturn -\nEINVAL;\n\nf = fdget(pidfd);\nif (!f.file)\nreturn -\nEBADF;\n\npid = pidfd_to_pid(f.file);\nif (\nIS_ERR(pid)\n) {\nret = PTR_ERR(pid);\ngoto\nerr;\n}\n\nret = -EINVAL;\nif (!\naccess_pidfd_pidns(pid)\n)\ngoto\nerr;\n\nif (info) {\nret = copy_siginfo_from_user_any(&kinfo, info);\nif (\nunlikely(ret)\n)\ngoto\nerr;\n\nret = -EINVAL;\nif (\nunlikely(sig\n!= kinfo.si_signo))\ngoto\nerr;\n\nret = -EPERM;\nif ((\ntask_pid(current)\n!= pid) &&\n(kinfo.si_code >= 0 || kinfo.si_code == SI_TKILL))\ngoto\nerr;\n} else {\nprepare_kill_siginfo(sig, &kinfo\n);\n}\n\nret = kill_pid_info(sig, &kinfo, pid);\n\nerr:\nfdput(f);\nreturn\nret;\n}\n\nstatic int\n    do_send_specific(pid_t\ntgid,\npid_t pid,\nint sig,\nstruct kernel_siginfo *info\n)\n{\nstruct task_struct *p;\nint error = -ESRCH;\n\nrcu_read_lock();\n\np = find_task_by_vpid(pid);\nif (\np &&(tgid\n<= 0 ||\ntask_tgid_vnr(p)\n== tgid)) {\nerror = check_kill_permission(sig, info, p);\n\nif (!\nerror &&sig\n) {\nerror = do_send_sig_info(sig, info, p, PIDTYPE_PID);\n\nif (\nunlikely(error\n== -ESRCH))\nerror = 0;\n}\n}\n\nrcu_read_unlock();\n\nreturn\nerror;\n}\n\nstatic int do_tkill(pid_t\ntgid,\npid_t pid,\nint sig\n)\n{\nstruct kernel_siginfo info;\n\nclear_siginfo(&info);\ninfo.\nsi_signo = sig;\ninfo.\nsi_errno = 0;\ninfo.\nsi_code = SI_TKILL;\ninfo.\nsi_pid = task_tgid_vnr(current);\ninfo.\nsi_uid = from_kuid_munged(current_user_ns(), current_uid());\n\nreturn\ndo_send_specific(tgid, pid, sig, &info\n);\n}\nSYSCALL_DEFINE3(tgkill, pid_t, tgid, pid_t, pid,\nint, sig)\n{\n\nif (pid <= 0 || tgid <= 0)\nreturn -\nEINVAL;\n\nreturn\ndo_tkill(tgid, pid, sig\n);\n}\nSYSCALL_DEFINE2(tkill, pid_t, pid,\nint, sig)\n{\n\nif (pid <= 0)\nreturn -\nEINVAL;\n\nreturn do_tkill(0, pid, sig);\n}\n\nstatic int do_rt_sigqueueinfo(pid_t\npid,\nint sig, kernel_siginfo_t\n*info)\n{\n\nif ((info->si_code >= 0 || info->si_code == SI_TKILL) &&\n(\ntask_pid_vnr(current)\n!= pid))\nreturn -\nEPERM;\n\nreturn\nkill_proc_info(sig, info, pid\n);\n}\n\nSYSCALL_DEFINE3(rt_sigqueueinfo, pid_t, pid,\nint, sig,\nsiginfo_t __user\n*, uinfo)\n{\nkernel_siginfo_t info;\nint ret = __copy_siginfo_from_user(sig, &info, uinfo);\nif (\nunlikely(ret)\n)\nreturn\nret;\nreturn\ndo_rt_sigqueueinfo(pid, sig, &info\n);\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE3(rt_sigqueueinfo,\n      compat_pid_t, pid,\n      int, sig,\n      struct compat_siginfo __user *, uinfo)\n{\n  kernel_siginfo_t info;\n  int ret = __copy_siginfo_from_user32(sig, &info, uinfo);\n  if (unlikely(ret))\n    return ret;\n  return do_rt_sigqueueinfo(pid, sig, &info);\n}\n#endif\n\nstatic int do_rt_tgsigqueueinfo(pid_t\ntgid,\npid_t pid,\nint sig, kernel_siginfo_t\n*info)\n{\n\nif (pid <= 0 || tgid <= 0)\nreturn -\nEINVAL;\n\nif ((info->si_code >= 0 || info->si_code == SI_TKILL) &&\n(\ntask_pid_vnr(current)\n!= pid))\nreturn -\nEPERM;\n\nreturn\ndo_send_specific(tgid, pid, sig, info\n);\n}\n\nSYSCALL_DEFINE4(rt_tgsigqueueinfo, pid_t, tgid, pid_t, pid,\nint, sig,\nsiginfo_t __user\n*, uinfo)\n{\nkernel_siginfo_t info;\nint ret = __copy_siginfo_from_user(sig, &info, uinfo);\nif (\nunlikely(ret)\n)\nreturn\nret;\nreturn\ndo_rt_tgsigqueueinfo(tgid, pid, sig, &info\n);\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE4(rt_tgsigqueueinfo,\n      compat_pid_t, tgid,\n      compat_pid_t, pid,\n      int, sig,\n      struct compat_siginfo __user *, uinfo)\n{\n  kernel_siginfo_t info;\n  int ret = __copy_siginfo_from_user32(sig, &info, uinfo);\n  if (unlikely(ret))\n    return ret;\n  return do_rt_tgsigqueueinfo(tgid, pid, sig, &info);\n}\n#endif\n\nvoid kernel_sigaction(int sig, __sighandler_t action) {\n  spin_lock_irq(&current->sighand->siglock);\n  current->sighand->action[sig - 1].sa.sa_handler = action;\n  if (action == SIG_IGN) {\n    sigset_t mask;\n\n    sigemptyset(&mask);\n    sigaddset(&mask, sig);\n\n    flush_sigqueue_mask(&mask, &current->signal->shared_pending);\n    flush_sigqueue_mask(&mask, &current->pending);\n    recalc_sigpending();\n  }\n  spin_unlock_irq(&current->sighand->siglock);\n}\n\nEXPORT_SYMBOL(kernel_sigaction);\n\nvoid __weak\n\nsigaction_compat_abi(struct k_sigaction *act,\n                     struct k_sigaction *oact) {\n}\n\nint do_sigaction(int sig, struct k_sigaction *act, struct k_sigaction *oact) {\n  struct task_struct *p = current, *t;\n  struct k_sigaction *k;\n  sigset_t mask;\n\n  if (!valid_signal(sig) || sig < 1 || (act && sig_kernel_only(sig)))\n    return -EINVAL;\n\n  k = &p->sighand->action[sig - 1];\n\n  spin_lock_irq(&p->sighand->siglock);\n  if (k->sa.sa_flags & SA_IMMUTABLE) {\n    spin_unlock_irq(&p->sighand->siglock);\n    return -EINVAL;\n  }\n  if (oact)\n    *oact = *k;\n\n  BUILD_BUG_ON(UAPI_SA_FLAGS & SA_UNSUPPORTED);\n\n  if (act)\n    act->sa.sa_flags &= UAPI_SA_FLAGS;\n  if (oact)\n    oact->sa.sa_flags &= UAPI_SA_FLAGS;\n\n  sigaction_compat_abi(act, oact);\n\n  if (act) {\n    sigdelsetmask(&act->sa.sa_mask,\n                  sigmask(SIGKILL) | sigmask(SIGSTOP));\n    *k = *act;\n    if (sig_handler_ignored(sig_handler(p, sig), sig)) {\n      sigemptyset(&mask);\n      sigaddset(&mask, sig);\n      flush_sigqueue_mask(&mask, &p->signal->shared_pending);\n      for_each_thread(p, t)\n      flush_sigqueue_mask(&mask, &t->pending);\n    }\n  }\n\n  spin_unlock_irq(&p->sighand->siglock);\n  return 0;\n}\n\n#ifdef CONFIG_DYNAMIC_SIGFRAME\nstatic inline void sigaltstack_lock(void)\n  __acquires(&current->sighand->siglock)\n{\n  spin_lock_irq(&current->sighand->siglock);\n}\n\nstatic inline void sigaltstack_unlock(void)\n  __releases(&current->sighand->siglock)\n{\n  spin_unlock_irq(&current->sighand->siglock);\n}\n#else\n\nstatic inline void sigaltstack_lock(void) {}\n\nstatic inline void sigaltstack_unlock(void) {}\n\n#endif\n\nstatic int\ndo_sigaltstack(const stack_t *ss, stack_t *oss, unsigned long sp,\n               size_t min_ss_size) {\n  struct task_struct *t = current;\n  int ret = 0;\n\n  if (oss) {\n    memset(oss, 0, sizeof(stack_t));\n    oss->ss_sp = (void\n    __user *) t->sas_ss_sp;\n    oss->ss_size = t->sas_ss_size;\n    oss->ss_flags = sas_ss_flags(sp) |\n                    (current->sas_ss_flags & SS_FLAG_BITS);\n  }\n\n  if (ss) {\n    void __user\n    *ss_sp = ss->ss_sp;\n    size_t ss_size = ss->ss_size;\n    unsigned ss_flags = ss->ss_flags;\n    int ss_mode;\n\n    if (unlikely(on_sig_stack(sp)))\n      return -EPERM;\n\n    ss_mode = ss_flags & ~SS_FLAG_BITS;\n    if (unlikely(ss_mode != SS_DISABLE && ss_mode != SS_ONSTACK &&\n                 ss_mode != 0))\n      return -EINVAL;\n\n    if (t->sas_ss_sp == (unsigned long) ss_sp &&\n        t->sas_ss_size == ss_size &&\n        t->sas_ss_flags == ss_flags)\n      return 0;\n\n    sigaltstack_lock();\n    if (ss_mode == SS_DISABLE) {\n      ss_size = 0;\n      ss_sp = NULL;\n    } else {\n      if (unlikely(ss_size < min_ss_size))\n        ret = -ENOMEM;\n      if (!sigaltstack_size_valid(ss_size))\n        ret = -ENOMEM;\n    }\n    if (!ret) {\n      t->sas_ss_sp = (unsigned long) ss_sp;\n      t->sas_ss_size = ss_size;\n      t->sas_ss_flags = ss_flags;\n    }\n    sigaltstack_unlock();\n  }\n  return ret;\n}\n\nSYSCALL_DEFINE2(sigaltstack,\nconst stack_t __user\n*,uss,\nstack_t __user\n*,uoss)\n{\nstack_t new,\nold;\nint err;\nif (\nuss &&copy_from_user(&new, uss, sizeof(stack_t))\n)\nreturn -\nEFAULT;\nerr = do_sigaltstack(uss ? &new : NULL, uoss ? &old : NULL,\n                     current_user_stack_pointer(),\n                     MINSIGSTKSZ);\nif (!\nerr &&uoss\n&&\ncopy_to_user(uoss, &old,\nsizeof(stack_t)))\nerr = -EFAULT;\nreturn\nerr;\n}\n\nint restore_altstack(const stack_t __user\n\n*uss)\n{\nstack_t new;\nif (copy_from_user(&new, uss, sizeof(stack_t)))\nreturn -\nEFAULT;\n(void)do_sigaltstack(&new, NULL,\n\ncurrent_user_stack_pointer(),\n    MINSIGSTKSZ\n\n);\n\nreturn 0;\n}\n\nint __save_altstack(stack_t __user\n\n*uss,\nunsigned long sp\n)\n{\nstruct task_struct *t = current;\nint err = __put_user((void\n__user *)t->sas_ss_sp, &uss->ss_sp) |\n__put_user(t\n->sas_ss_flags, &uss->ss_flags) |\n__put_user(t\n->sas_ss_size, &uss->ss_size);\nreturn\nerr;\n}\n\n#ifdef CONFIG_COMPAT\nstatic int do_compat_sigaltstack(const compat_stack_t __user *uss_ptr,\n         compat_stack_t __user *uoss_ptr)\n{\n  stack_t uss, uoss;\n  int ret;\n\n  if (uss_ptr) {\n    compat_stack_t uss32;\n    if (copy_from_user(&uss32, uss_ptr, sizeof(compat_stack_t)))\n      return -EFAULT;\n    uss.ss_sp = compat_ptr(uss32.ss_sp);\n    uss.ss_flags = uss32.ss_flags;\n    uss.ss_size = uss32.ss_size;\n  }\n  ret = do_sigaltstack(uss_ptr ? &uss : NULL, &uoss,\n           compat_user_stack_pointer(),\n           COMPAT_MINSIGSTKSZ);\n  if (ret >= 0 && uoss_ptr) {\n    compat_stack_t old;\n    memset(&old, 0, sizeof(old));\n    old.ss_sp = ptr_to_compat(uoss.ss_sp);\n    old.ss_flags = uoss.ss_flags;\n    old.ss_size = uoss.ss_size;\n    if (copy_to_user(uoss_ptr, &old, sizeof(compat_stack_t)))\n      ret = -EFAULT;\n  }\n  return ret;\n}\n\nCOMPAT_SYSCALL_DEFINE2(sigaltstack,\n      const compat_stack_t __user *, uss_ptr,\n      compat_stack_t __user *, uoss_ptr)\n{\n  return do_compat_sigaltstack(uss_ptr, uoss_ptr);\n}\n\nint compat_restore_altstack(const compat_stack_t __user *uss)\n{\n  int err = do_compat_sigaltstack(uss, NULL);\n\n  return err == -EFAULT ? err : 0;\n}\n\nint __compat_save_altstack(compat_stack_t __user *uss, unsigned long sp)\n{\n  int err;\n  struct task_struct *t = current;\n  err = __put_user(ptr_to_compat((void __user *)t->sas_ss_sp),\n       &uss->ss_sp) |\n    __put_user(t->sas_ss_flags, &uss->ss_flags) |\n    __put_user(t->sas_ss_size, &uss->ss_size);\n  return err;\n}\n#endif\n\n#ifdef __ARCH_WANT_SYS_SIGPENDING\n\n\n\n\n\nSYSCALL_DEFINE1(sigpending, old_sigset_t __user *, uset)\n{\n  sigset_t set;\n\n  if (sizeof(old_sigset_t) > sizeof(*uset))\n    return -EINVAL;\n\n  do_sigpending(&set);\n\n  if (copy_to_user(uset, &set, sizeof(old_sigset_t)))\n    return -EFAULT;\n\n  return 0;\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE1(sigpending, compat_old_sigset_t __user *, set32)\n{\n  sigset_t set;\n\n  do_sigpending(&set);\n\n  return put_user(set.sig[0], set32);\n}\n#endif\n\n#endif\n\n#ifdef __ARCH_WANT_SYS_SIGPROCMASK\n\nSYSCALL_DEFINE3(sigprocmask, int, how, old_sigset_t __user *, nset,\n    old_sigset_t __user *, oset)\n{\n  old_sigset_t old_set, new_set;\n  sigset_t new_blocked;\n\n  old_set = current->blocked.sig[0];\n\n  if (nset) {\n    if (copy_from_user(&new_set, nset, sizeof(*nset)))\n      return -EFAULT;\n\n    new_blocked = current->blocked;\n\n    switch (how) {\n    case SIG_BLOCK:\n      sigaddsetmask(&new_blocked, new_set);\n      break;\n    case SIG_UNBLOCK:\n      sigdelsetmask(&new_blocked, new_set);\n      break;\n    case SIG_SETMASK:\n      new_blocked.sig[0] = new_set;\n      break;\n    default:\n      return -EINVAL;\n    }\n\n    set_current_blocked(&new_blocked);\n  }\n\n  if (oset) {\n    if (copy_to_user(oset, &old_set, sizeof(*oset)))\n      return -EFAULT;\n  }\n\n  return 0;\n}\n#endif\n\n#ifndef CONFIG_ODD_RT_SIGACTION\n\nSYSCALL_DEFINE4(rt_sigaction,\nint, sig,\nconst struct sigaction __user\n*, act,\nstruct sigaction __user\n*, oact,\nsize_t, sigsetsize)\n{\nstruct k_sigaction new_sa, old_sa;\nint ret;\n\nif (sigsetsize != sizeof(sigset_t))\nreturn -\nEINVAL;\n\nif (\nact &&copy_from_user(&new_sa.sa, act, sizeof(new_sa.sa))\n)\nreturn -\nEFAULT;\n\nret = do_sigaction(sig, act ? &new_sa : NULL, oact ? &old_sa : NULL);\nif (ret)\nreturn\nret;\n\nif (\noact &&copy_to_user(oact, &old_sa.sa, sizeof(old_sa.sa))\n)\nreturn -\nEFAULT;\n\nreturn 0;\n}\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE4(rt_sigaction, int, sig,\n    const struct compat_sigaction __user *, act,\n    struct compat_sigaction __user *, oact,\n    compat_size_t, sigsetsize)\n{\n  struct k_sigaction new_ka, old_ka;\n#ifdef __ARCH_HAS_SA_RESTORER\n  compat_uptr_t restorer;\n#endif\n  int ret;\n\n\n  if (sigsetsize != sizeof(compat_sigset_t))\n    return -EINVAL;\n\n  if (act) {\n    compat_uptr_t handler;\n    ret = get_user(handler, &act->sa_handler);\n    new_ka.sa.sa_handler = compat_ptr(handler);\n#ifdef __ARCH_HAS_SA_RESTORER\n    ret |= get_user(restorer, &act->sa_restorer);\n    new_ka.sa.sa_restorer = compat_ptr(restorer);\n#endif\n    ret |= get_compat_sigset(&new_ka.sa.sa_mask, &act->sa_mask);\n    ret |= get_user(new_ka.sa.sa_flags, &act->sa_flags);\n    if (ret)\n      return -EFAULT;\n  }\n\n  ret = do_sigaction(sig, act ? &new_ka : NULL, oact ? &old_ka : NULL);\n  if (!ret && oact) {\n    ret = put_user(ptr_to_compat(old_ka.sa.sa_handler),\n             &oact->sa_handler);\n    ret |= put_compat_sigset(&oact->sa_mask, &old_ka.sa.sa_mask,\n           sizeof(oact->sa_mask));\n    ret |= put_user(old_ka.sa.sa_flags, &oact->sa_flags);\n#ifdef __ARCH_HAS_SA_RESTORER\n    ret |= put_user(ptr_to_compat(old_ka.sa.sa_restorer),\n        &oact->sa_restorer);\n#endif\n  }\n  return ret;\n}\n#endif\n#endif\n\n#ifdef CONFIG_OLD_SIGACTION\nSYSCALL_DEFINE3(sigaction, int, sig,\n    const struct old_sigaction __user *, act,\n          struct old_sigaction __user *, oact)\n{\n  struct k_sigaction new_ka, old_ka;\n  int ret;\n\n  if (act) {\n    old_sigset_t mask;\n    if (!access_ok(act, sizeof(*act)) ||\n        __get_user(new_ka.sa.sa_handler, &act->sa_handler) ||\n        __get_user(new_ka.sa.sa_restorer, &act->sa_restorer) ||\n        __get_user(new_ka.sa.sa_flags, &act->sa_flags) ||\n        __get_user(mask, &act->sa_mask))\n      return -EFAULT;\n#ifdef __ARCH_HAS_KA_RESTORER\n    new_ka.ka_restorer = NULL;\n#endif\n    siginitset(&new_ka.sa.sa_mask, mask);\n  }\n\n  ret = do_sigaction(sig, act ? &new_ka : NULL, oact ? &old_ka : NULL);\n\n  if (!ret && oact) {\n    if (!access_ok(oact, sizeof(*oact)) ||\n        __put_user(old_ka.sa.sa_handler, &oact->sa_handler) ||\n        __put_user(old_ka.sa.sa_restorer, &oact->sa_restorer) ||\n        __put_user(old_ka.sa.sa_flags, &oact->sa_flags) ||\n        __put_user(old_ka.sa.sa_mask.sig[0], &oact->sa_mask))\n      return -EFAULT;\n  }\n\n  return ret;\n}\n#endif\n#ifdef CONFIG_COMPAT_OLD_SIGACTION\nCOMPAT_SYSCALL_DEFINE3(sigaction, int, sig,\n    const struct compat_old_sigaction __user *, act,\n          struct compat_old_sigaction __user *, oact)\n{\n  struct k_sigaction new_ka, old_ka;\n  int ret;\n  compat_old_sigset_t mask;\n  compat_uptr_t handler, restorer;\n\n  if (act) {\n    if (!access_ok(act, sizeof(*act)) ||\n        __get_user(handler, &act->sa_handler) ||\n        __get_user(restorer, &act->sa_restorer) ||\n        __get_user(new_ka.sa.sa_flags, &act->sa_flags) ||\n        __get_user(mask, &act->sa_mask))\n      return -EFAULT;\n\n#ifdef __ARCH_HAS_KA_RESTORER\n    new_ka.ka_restorer = NULL;\n#endif\n    new_ka.sa.sa_handler = compat_ptr(handler);\n    new_ka.sa.sa_restorer = compat_ptr(restorer);\n    siginitset(&new_ka.sa.sa_mask, mask);\n  }\n\n  ret = do_sigaction(sig, act ? &new_ka : NULL, oact ? &old_ka : NULL);\n\n  if (!ret && oact) {\n    if (!access_ok(oact, sizeof(*oact)) ||\n        __put_user(ptr_to_compat(old_ka.sa.sa_handler),\n             &oact->sa_handler) ||\n        __put_user(ptr_to_compat(old_ka.sa.sa_restorer),\n             &oact->sa_restorer) ||\n        __put_user(old_ka.sa.sa_flags, &oact->sa_flags) ||\n        __put_user(old_ka.sa.sa_mask.sig[0], &oact->sa_mask))\n      return -EFAULT;\n  }\n  return ret;\n}\n#endif\n\n#ifdef CONFIG_SGETMASK_SYSCALL\n\n\n\n\nSYSCALL_DEFINE0(sgetmask)\n{\n\n  return current->blocked.sig[0];\n}\n\nSYSCALL_DEFINE1(ssetmask, int, newmask)\n{\n  int old = current->blocked.sig[0];\n  sigset_t newset;\n\n  siginitset(&newset, newmask);\n  set_current_blocked(&newset);\n\n  return old;\n}\n#endif\n\n#ifdef __ARCH_WANT_SYS_SIGNAL\n\n\n\nSYSCALL_DEFINE2(signal, int, sig, __sighandler_t, handler)\n{\n  struct k_sigaction new_sa, old_sa;\n  int ret;\n\n  new_sa.sa.sa_handler = handler;\n  new_sa.sa.sa_flags = SA_ONESHOT | SA_NOMASK;\n  sigemptyset(&new_sa.sa.sa_mask);\n\n  ret = do_sigaction(sig, &new_sa, &old_sa);\n\n  return ret ? ret : (unsigned long)old_sa.sa.sa_handler;\n}\n#endif\n\n#ifdef __ARCH_WANT_SYS_PAUSE\n\nSYSCALL_DEFINE0(pause)\n{\n  while (!signal_pending(current)) {\n    __set_current_state(TASK_INTERRUPTIBLE);\n    schedule();\n  }\n  return -ERESTARTNOHAND;\n}\n\n#endif\n\nstatic int sigsuspend(sigset_t * set) {\n  current->saved_sigmask = current->blocked;\n  set_current_blocked(set);\n\n  while (!signal_pending(current)) {\n    __set_current_state(TASK_INTERRUPTIBLE);\n    schedule();\n  }\n  set_restore_sigmask();\n  return -ERESTARTNOHAND;\n}\n\nSYSCALL_DEFINE2(rt_sigsuspend, sigset_t\n__user *, unewset, size_t, sigsetsize)\n{\nsigset_t newset;\n\nif (sigsetsize != sizeof(sigset_t))\nreturn -\nEINVAL;\n\nif (\ncopy_from_user(&newset, unewset,\nsizeof(newset)))\nreturn -\nEFAULT;\nreturn\nsigsuspend(&newset);\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE2(rt_sigsuspend, compat_sigset_t __user *, unewset, compat_s\n{\n  sigset_t newset;\n\n\n  if (sigsetsize != sizeof(sigset_t))\n    return -EINVAL;\n\n  if (get_compat_sigset(&newset, unewset))\n    return -EFAULT;\n  return sigsuspend(&newset);\n}\n#endif\n\n#ifdef CONFIG_OLD_SIGSUSPEND\nSYSCALL_DEFINE1(sigsuspend, old_sigset_t, mask)\n{\n  sigset_t blocked;\n  siginitset(&blocked, mask);\n  return sigsuspend(&blocked);\n}\n#endif\n#ifdef CONFIG_OLD_SIGSUSPEND3\nSYSCALL_DEFINE3(sigsuspend, int, unused1, int, unused2, old_sigset_t, mask)\n{\n  sigset_t blocked;\n  siginitset(&blocked, mask);\n  return sigsuspend(&blocked);\n}\n#endif\n\n__weak const char *arch_vma_name(struct vm_area_struct *vma) {\n  return NULL;\n}\n\nstatic inline void siginfo_buildtime_checks(void) {\n  BUILD_BUG_ON(sizeof(struct siginfo) != SI_MAX_SIZE);\n\n#define CHECK_OFFSET(field) \\\n  BUILD_BUG_ON(offsetof(siginfo_t, field) != offsetof(kernel_siginfo_t, field))\n\n  CHECK_OFFSET(si_pid);\n  CHECK_OFFSET(si_uid);\n\n  CHECK_OFFSET(si_tid);\n  CHECK_OFFSET(si_overrun);\n  CHECK_OFFSET(si_value);\n\n  CHECK_OFFSET(si_pid);\n  CHECK_OFFSET(si_uid);\n  CHECK_OFFSET(si_value);\n\n  CHECK_OFFSET(si_pid);\n  CHECK_OFFSET(si_uid);\n  CHECK_OFFSET(si_status);\n  CHECK_OFFSET(si_utime);\n  CHECK_OFFSET(si_stime);\n\n  CHECK_OFFSET(si_addr);\n  CHECK_OFFSET(si_trapno);\n  CHECK_OFFSET(si_addr_lsb);\n  CHECK_OFFSET(si_lower);\n  CHECK_OFFSET(si_upper);\n  CHECK_OFFSET(si_pkey);\n  CHECK_OFFSET(si_perf_data);\n  CHECK_OFFSET(si_perf_type);\n\n  CHECK_OFFSET(si_band);\n  CHECK_OFFSET(si_fd);\n\n  CHECK_OFFSET(si_call_addr);\n  CHECK_OFFSET(si_syscall);\n  CHECK_OFFSET(si_arch);\n#undef CHECK_OFFSET\n\n  BUILD_BUG_ON(offsetof(\n  struct siginfo, si_pid) !=\n  offsetof(\n  struct siginfo, si_addr));\n  if (sizeof(int) == sizeof(void __user *)) {\n    BUILD_BUG_ON(sizeof_field(\n    struct siginfo, si_pid) !=\n    sizeof(void\n    __user *));\n  } else {\n    BUILD_BUG_ON((sizeof_field(\n    struct siginfo, si_pid) +\n        sizeof_field(\n    struct siginfo, si_uid)) !=\n    sizeof(void\n    __user *));\n    BUILD_BUG_ON(offsetofend(\n    struct siginfo, si_pid) !=\n    offsetof(\n    struct siginfo, si_uid));\n  }\n#ifdef CONFIG_COMPAT\n  BUILD_BUG_ON(offsetof(struct compat_siginfo, si_pid) !=\n         offsetof(struct compat_siginfo, si_addr));\n  BUILD_BUG_ON(sizeof_field(struct compat_siginfo, si_pid) !=\n         sizeof(compat_uptr_t));\n  BUILD_BUG_ON(sizeof_field(struct compat_siginfo, si_pid) !=\n         sizeof_field(struct siginfo, si_pid));\n#endif\n}\n\nvoid __init\n\nsignals_init(void) {\n  siginfo_buildtime_checks();\n\n  sigqueue_cachep = KMEM_CACHE(sigqueue, SLAB_PANIC | SLAB_ACCOUNT);\n}\n\n#ifdef CONFIG_KGDB_KDB\n#include <linux/kdb.h>\n\n\n\n\n\n\nvoid kdb_send_sig(struct task_struct *t, int sig)\n{\n  static struct task_struct *kdb_prev_t;\n  int new_t, ret;\n  if (!spin_trylock(&t->sighand->siglock)) {\n    kdb_printf("Can\'t do kill command now.\\n"\n         "The sigmask lock is held somewhere else in "\n         "kernel, try again later\\n");\n    return;\n  }\n  new_t = kdb_prev_t != t;\n  kdb_prev_t = t;\n  if (!task_is_running(t) && new_t) {\n    spin_unlock(&t->sighand->siglock);\n    kdb_printf("Process is not RUNNING, sending a signal from "\n         "kdb risks deadlock\\n"\n         "on the run queue locks. "\n         "The signal has _not_ been sent.\\n"\n         "Reissue the kill command if you want to risk "\n         "the deadlock.\\n");\n    return;\n  }\n  ret = send_signal(sig, SEND_SIG_PRIV, t, PIDTYPE_PID);\n  spin_unlock(&t->sighand->siglock);\n  if (ret)\n    kdb_printf("Fail to deliver Signal %d to process %d.\\n",\n         sig, t->pid);\n  else\n    kdb_printf("Signal %d is sent to process %d.\\n", sig, t->pid);\n}\n#endif\n';

const linux = {
  numCols: 81,
  get streams() {
    return [
      {
        get title() {
          delete this.title;
          const titles = ["linux - signal.c", "Editor - C", "C"];
          return (this.title = randomItemFromArray(titles));
        },

        get stream() {
          delete this.stream;
          return (this.stream = randomLineRepeatedString(stream));
        },

        get dt() {
          delete this.dt;
          return (this.dt = 81 * 5);
        },
      },
    ];
  },
};

export default linux;
